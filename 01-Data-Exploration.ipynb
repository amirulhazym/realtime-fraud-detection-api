{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc64539-196b-4e67-b787-f78718efe6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (4.13.0)\n",
      "Requirement already satisfied: colorama in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247332bd-fcb3-49db-9495-11035a90ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load full dataset from: hf://datasets/MatrixIA/FraudData/FraudData.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 62075729-1f3a-4f5f-bf17-2f9fc0254e52)')' thrown while requesting GET https://huggingface.co/datasets/MatrixIA/FraudData/resolve/main/FraudData.csv\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded full dataset into Pandas DataFrame.\n",
      "Full DataFrame shape: (6362620, 11)\n",
      "Creating a sample of 1000000 rows for initial development...\n",
      "Sample DataFrame shape: (1000000, 11)\n",
      "\n",
      "First 5 rows (from sample):\n",
      "         step      type      amount     nameOrig  oldbalanceOrg  \\\n",
      "5035970   354   CASH_IN   144975.52   C175979326        22432.0   \n",
      "5893406   403  CASH_OUT   265951.92   C612269677            0.0   \n",
      "5738504   399  TRANSFER  2112079.70  C1733159827            0.0   \n",
      "998697     45  CASH_OUT    46140.90  C1043238312        12226.0   \n",
      "1371340   138   CASH_IN   146372.57   C926661406       115404.0   \n",
      "\n",
      "         newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  \\\n",
      "5035970       167407.52  C2038108476            0.00            0.00        0   \n",
      "5893406            0.00  C2108977884      1896130.67      2162082.59        0   \n",
      "5738504            0.00   C440981795      3695546.40      5807626.10        0   \n",
      "998697             0.00  C1614628724       952567.92       998708.82        0   \n",
      "1371340       261776.57  C1593725318        24368.58            0.00        0   \n",
      "\n",
      "         isFlaggedFraud  \n",
      "5035970               0  \n",
      "5893406               0  \n",
      "5738504               0  \n",
      "998697                0  \n",
      "1371340               0  \n",
      "\n",
      "DataFrame Info (from sample):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000000 entries, 5035970 to 6154769\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   type            1000000 non-null  object \n",
      " 2   amount          1000000 non-null  float64\n",
      " 3   nameOrig        1000000 non-null  object \n",
      " 4   oldbalanceOrg   1000000 non-null  float64\n",
      " 5   newbalanceOrig  1000000 non-null  float64\n",
      " 6   nameDest        1000000 non-null  object \n",
      " 7   oldbalanceDest  1000000 non-null  float64\n",
      " 8   newbalanceDest  1000000 non-null  float64\n",
      " 9   isFraud         1000000 non-null  int64  \n",
      " 10  isFlaggedFraud  1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 91.6+ MB\n",
      "\n",
      "Summary Statistics (from sample):\n",
      "                 step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  1000000.000000  1.000000e+06   1.000000e+06    1.000000e+06   \n",
      "mean       243.371802  1.797594e+05   8.299145e+05    8.510678e+05   \n",
      "std        142.260556  6.062601e+05   2.871938e+06    2.907873e+06   \n",
      "min          1.000000  0.000000e+00   0.000000e+00    0.000000e+00   \n",
      "25%        155.000000  1.346105e+04   0.000000e+00    0.000000e+00   \n",
      "50%        239.000000  7.529329e+04   1.409840e+04    0.000000e+00   \n",
      "75%        335.000000  2.088129e+05   1.072451e+05    1.441033e+05   \n",
      "max        743.000000  6.329484e+07   3.856340e+07    3.893942e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest         isFraud  isFlaggedFraud  \n",
      "count    1.000000e+06    1.000000e+06  1000000.000000  1000000.000000  \n",
      "mean     1.100066e+06    1.224060e+06        0.001331        0.000005  \n",
      "std      3.417339e+06    3.683874e+06        0.036459        0.002236  \n",
      "min      0.000000e+00    0.000000e+00        0.000000        0.000000  \n",
      "25%      0.000000e+00    0.000000e+00        0.000000        0.000000  \n",
      "50%      1.340857e+05    2.162969e+05        0.000000        0.000000  \n",
      "75%      9.409902e+05    1.112023e+06        0.000000        0.000000  \n",
      "max      3.560159e+08    3.561793e+08        1.000000        1.000000  \n",
      "\n",
      "Missing Values Count per Column (from sample):\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution for Target Column ('isFraud') in Sample:\n",
      "isFraud\n",
      "0    0.998669\n",
      "1    0.001331\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Raw Counts in Sample:\n",
      "isFraud\n",
      "0    998669\n",
      "1      1331\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the special Hugging Face URL for the CSV file\n",
    "# This tells pandas to use fsspec and huggingface_hub to find and read the file\n",
    "hf_csv_url = \"hf://datasets/MatrixIA/FraudData/FraudData.csv\"\n",
    "\n",
    "print(f\"Attempting to load full dataset from: {hf_csv_url}\")\n",
    "try:\n",
    "    # Use pandas read_csv directly with the hf:// URL\n",
    "    # This loads the entire dataset into memory. May take a minute or two.\n",
    "    df_full = pd.read_csv(hf_csv_url)\n",
    "    print(\"Successfully loaded full dataset into Pandas DataFrame.\")\n",
    "    print(f\"Full DataFrame shape: {df_full.shape}\")\n",
    "\n",
    "    # --- IMPORTANT: Create a Sample for Development ---\n",
    "    # Define sample size (e.g., 1 million rows)\n",
    "    sample_size = 1000000\n",
    "    print(f\"Creating a sample of {sample_size} rows for initial development...\")\n",
    "\n",
    "    # Option 1: Take the first N rows (simplest)\n",
    "    df = df_full.head(sample_size).copy()\n",
    "\n",
    "    # Option 2: Take a random sample (better representation, might be slightly slower)\n",
    "    df = df_full.sample(n=sample_size, random_state=50).copy()\n",
    "\n",
    "    print(f\"Sample DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # Optional: Delete the full dataframe to free memory if you notice slowdowns\n",
    "    # Although with 24GB RAM, it might not be necessary yet.\n",
    "    # del df_full\n",
    "    # import gc # Garbage collector\n",
    "    # gc.collect() # Force memory cleanup\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch potential errors during download or reading\n",
    "    print(f\"ERROR: Failed to load dataset using pd.read_csv('hf://...'). Error: {e}\")\n",
    "    print(\"Check your internet connection, proxy settings (if any), and the URL.\")\n",
    "    raise # Stop execution\n",
    "\n",
    "# --- Basic Inspection (Run on the SAMPLE 'df') ---\n",
    "# This part remains the same as before, using the 'df' variable which now holds the sample\n",
    "print(\"\\nFirst 5 rows (from sample):\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataFrame Info (from sample):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary Statistics (from sample):\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values Count per Column (from sample):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# --- Target Variable Check (Run on the SAMPLE 'df') ---\n",
    "# Target column is 'isFraud' for this dataset (PaySim)\n",
    "target_column = 'isFraud'\n",
    "\n",
    "if target_column in df.columns:\n",
    "    print(f\"\\nClass Distribution for Target Column ('{target_column}') in Sample:\")\n",
    "    print(df[target_column].value_counts(normalize=True))\n",
    "    print(\"\\nRaw Counts in Sample:\")\n",
    "    print(df[target_column].value_counts(normalize=False))\n",
    "else:\n",
    "    print(f\"\\nERROR: Target column '{target_column}' not found in the DataFrame!\")\n",
    "    print(f\"Available columns are: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f27c075-4799-4c72-927f-9f182e208fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating data profiling report on the SAMPLE... (This might take a minute or two)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dff2916282c40239f130e4fcf42c73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:16<00:00,  1.54s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f3f59730f041b696a7dcd18b0be63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cdb96bb05f4a76879d8bb5bed0378c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea3c121274d4aeca373b25a7e22d8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling report saved to fraud_data_profiling_report_sample.html\n"
     ]
    }
   ],
   "source": [
    "# Import the profiling tool\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pandas_profiling import ProfileReport\n",
    "    except ImportError:\n",
    "        print(\"ERROR: Neither ydata-profiling nor pandas-profiling seem to be installed.\")\n",
    "        print(\"Please run: pip install ydata-profiling\")\n",
    "        raise # Stop if library isn't installed\n",
    "\n",
    "print(\"\\nGenerating data profiling report on the SAMPLE... (This might take a minute or two)\")\n",
    "\n",
    "# Create the report object using the SAMPLE DataFrame 'df'\n",
    "profile = ProfileReport(df, title=\"Fraud Data Profiling Report (Sample)\", explorative=True)\n",
    "\n",
    "# Define the filename for the HTML report\n",
    "report_filename = \"fraud_data_profiling_report_sample.html\"\n",
    "\n",
    "# Save the report to an HTML file\n",
    "profile.to_file(report_filename)\n",
    "\n",
    "print(f\"Profiling report saved to {report_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850fcbf0-0359-45d1-939e-94c97cf48915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Step 2: Basic Preprocessing ---\n",
      "Original sample shape: (1000000, 11)\n",
      "Keeping features: ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
      "Target variable: isFraud\n",
      "\n",
      "Created df_processed with selected columns.\n",
      "Shape after selecting features: (1000000, 8)\n",
      "First 5 rows of df_processed:\n",
      "         step      type      amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "5035970   354   CASH_IN   144975.52        22432.0       167407.52   \n",
      "5893406   403  CASH_OUT   265951.92            0.0            0.00   \n",
      "5738504   399  TRANSFER  2112079.70            0.0            0.00   \n",
      "998697     45  CASH_OUT    46140.90        12226.0            0.00   \n",
      "1371340   138   CASH_IN   146372.57       115404.0       261776.57   \n",
      "\n",
      "         oldbalanceDest  newbalanceDest  isFraud  \n",
      "5035970            0.00            0.00        0  \n",
      "5893406      1896130.67      2162082.59        0  \n",
      "5738504      3695546.40      5807626.10        0  \n",
      "998697        952567.92       998708.82        0  \n",
      "1371340        24368.58            0.00        0  \n"
     ]
    }
   ],
   "source": [
    "#Step 2 Level 1: Preprocessing-and-Baseline\n",
    "# --- Step 2: Basic Preprocessing ---\n",
    "print(\"--- Starting Step 2: Basic Preprocessing ---\")\n",
    "\n",
    "# --- Step 2.1: Feature Selection ---\n",
    "# First, ensure 'df' holds the sample DataFrame from Step 1\n",
    "# (If in a new notebook, you might need to re-load/re-sample or load from a saved file)\n",
    "print(f\"Original sample shape: {df.shape}\")\n",
    "\n",
    "# Define the features we decided to keep\n",
    "features_to_keep = ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "target = 'isFraud' # Define the target column name\n",
    "\n",
    "print(f\"Keeping features: {features_to_keep}\")\n",
    "print(f\"Target variable: {target}\")\n",
    "\n",
    "try:\n",
    "    # Create a new DataFrame containing only the selected features and the target\n",
    "    # Using .copy() prevents accidental changes to the original 'df'\n",
    "    df_processed = df[features_to_keep + [target]].copy()\n",
    "\n",
    "    print(\"\\nCreated df_processed with selected columns.\")\n",
    "    print(f\"Shape after selecting features: {df_processed.shape}\")\n",
    "    print(\"First 5 rows of df_processed:\")\n",
    "    print(df_processed.head())\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A specified column was not found in the DataFrame: {e}\")\n",
    "    print(\"Please check the 'features_to_keep' list and the 'target' variable against the columns from df.info().\")\n",
    "    raise # Stop execution if columns are incorrect\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during feature selection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641233c4-ff29-4aa5-a442-8455fdbfc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types before encoding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000000 entries, 5035970 to 6154769\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   type            1000000 non-null  object \n",
      " 2   amount          1000000 non-null  float64\n",
      " 3   oldbalanceOrg   1000000 non-null  float64\n",
      " 4   newbalanceOrig  1000000 non-null  float64\n",
      " 5   oldbalanceDest  1000000 non-null  float64\n",
      " 6   newbalanceDest  1000000 non-null  float64\n",
      " 7   isFraud         1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 100.9+ MB\n",
      "None\n",
      "\n",
      "Unique values in 'type' column:\n",
      "['CASH_IN' 'CASH_OUT' 'TRANSFER' 'PAYMENT' 'DEBIT']\n",
      "\n",
      "Value counts for 'type':\n",
      "type\n",
      "CASH_OUT    352456\n",
      "PAYMENT     337709\n",
      "CASH_IN     219440\n",
      "TRANSFER     83906\n",
      "DEBIT         6489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying one-hot encoding to: ['type'] using pd.get_dummies...\n",
      "\n",
      "DataFrame after one-hot encoding:\n",
      "         step      amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "5035970   354   144975.52        22432.0       167407.52            0.00   \n",
      "5893406   403   265951.92            0.0            0.00      1896130.67   \n",
      "5738504   399  2112079.70            0.0            0.00      3695546.40   \n",
      "998697     45    46140.90        12226.0            0.00       952567.92   \n",
      "1371340   138   146372.57       115404.0       261776.57        24368.58   \n",
      "\n",
      "         newbalanceDest  isFraud  type_CASH_OUT  type_DEBIT  type_PAYMENT  \\\n",
      "5035970            0.00        0          False       False         False   \n",
      "5893406      2162082.59        0           True       False         False   \n",
      "5738504      5807626.10        0          False       False         False   \n",
      "998697        998708.82        0           True       False         False   \n",
      "1371340            0.00        0          False       False         False   \n",
      "\n",
      "         type_TRANSFER  \n",
      "5035970          False  \n",
      "5893406          False  \n",
      "5738504           True  \n",
      "998697           False  \n",
      "1371340          False  \n",
      "\n",
      "Shape after encoding: (1000000, 11)\n",
      "\n",
      "Data types after encoding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000000 entries, 5035970 to 6154769\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   amount          1000000 non-null  float64\n",
      " 2   oldbalanceOrg   1000000 non-null  float64\n",
      " 3   newbalanceOrig  1000000 non-null  float64\n",
      " 4   oldbalanceDest  1000000 non-null  float64\n",
      " 5   newbalanceDest  1000000 non-null  float64\n",
      " 6   isFraud         1000000 non-null  int64  \n",
      " 7   type_CASH_OUT   1000000 non-null  bool   \n",
      " 8   type_DEBIT      1000000 non-null  bool   \n",
      " 9   type_PAYMENT    1000000 non-null  bool   \n",
      " 10  type_TRANSFER   1000000 non-null  bool   \n",
      "dtypes: bool(4), float64(5), int64(2)\n",
      "memory usage: 97.1 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2.2: Encode Categorical Features ('type') ---\n",
    "\n",
    "# Identify the categorical column(s) remaining in df_processed\n",
    "categorical_cols = ['type']\n",
    "\n",
    "# Check current data types and unique values in 'type' before encoding\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(df_processed.info()) # Check df_processed specifically\n",
    "if 'type' in df_processed.columns:\n",
    "    print(f\"\\nUnique values in 'type' column:\\n{df_processed['type'].unique()}\")\n",
    "    print(f\"\\nValue counts for 'type':\\n{df_processed['type'].value_counts()}\")\n",
    "else:\n",
    "    print(\"\\n'type' column not found in df_processed (already dropped or renamed?).\")\n",
    "\n",
    "\n",
    "print(f\"\\nApplying one-hot encoding to: {categorical_cols} using pd.get_dummies...\")\n",
    "\n",
    "try:\n",
    "    # Use pd.get_dummies to convert the 'type' column\n",
    "    # drop_first=True removes redundancy (prevents multicollinearity)\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True) # This line reassigns df_processed\n",
    "\n",
    "    print(\"\\nDataFrame after one-hot encoding:\")\n",
    "    print(df_processed.head()) # Notice the original 'type' column is gone\n",
    "                               # and new 'type_TRANSFER', 'type_PAYMENT', etc. columns appear\n",
    "    print(f\"\\nShape after encoding: {df_processed.shape}\")\n",
    "\n",
    "    # Verify that all columns (except potentially target) are now numerical\n",
    "    print(\"\\nData types after encoding:\")\n",
    "    df_processed.info()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Column specified for encoding not found: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during one-hot encoding: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f7f52a-842b-472e-8cab-05b1199a4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.2.3: Separate Features (X) and Target (y) ---\n",
      "\n",
      "Separating features (X) and target ('isFraud')...\n",
      "Separation complete.\n",
      "\n",
      "Features (X) verification:\n",
      "  Shape: (1000000, 10)\n",
      "  Target column 'isFraud' successfully removed from X.\n",
      "  First 5 rows of X:\n",
      "         step      amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "5035970   354   144975.52        22432.0       167407.52            0.00   \n",
      "5893406   403   265951.92            0.0            0.00      1896130.67   \n",
      "5738504   399  2112079.70            0.0            0.00      3695546.40   \n",
      "998697     45    46140.90        12226.0            0.00       952567.92   \n",
      "1371340   138   146372.57       115404.0       261776.57        24368.58   \n",
      "\n",
      "         newbalanceDest  type_CASH_OUT  type_DEBIT  type_PAYMENT  \\\n",
      "5035970            0.00          False       False         False   \n",
      "5893406      2162082.59           True       False         False   \n",
      "5738504      5807626.10          False       False         False   \n",
      "998697        998708.82           True       False         False   \n",
      "1371340            0.00          False       False         False   \n",
      "\n",
      "         type_TRANSFER  \n",
      "5035970          False  \n",
      "5893406          False  \n",
      "5738504           True  \n",
      "998697           False  \n",
      "1371340          False  \n",
      "\n",
      "Target (y) verification:\n",
      "  Shape: (1000000,)\n",
      "  Data type: int64\n",
      "  First 5 values of y:\n",
      "5035970    0\n",
      "5893406    0\n",
      "5738504    0\n",
      "998697     0\n",
      "1371340    0\n",
      "Name: isFraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1.2.3: Separate Features (X) and Target (y) ---\n",
    "\n",
    "# Ensure the target variable name is correctly defined\n",
    "target = 'isFraud'\n",
    "\n",
    "print(\"\\n--- Step 1.2.3: Separate Features (X) and Target (y) ---\")\n",
    "print(f\"\\nSeparating features (X) and target ('{target}')...\")\n",
    "\n",
    "try:\n",
    "    # IMPORTANT: Make sure 'df_processed' is the DataFrame from the previous step\n",
    "    # containing the one-hot encoded 'type' columns.\n",
    "\n",
    "    # Create the features DataFrame 'X' by dropping the target column\n",
    "    # axis=1 specifies we are dropping a column\n",
    "    X = df_processed.drop(target, axis=1) # X should have 10 columns\n",
    "\n",
    "    # Create the target Series 'y' by selecting only the target column\n",
    "    y = df_processed[target] # y should be a Series\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(\"Separation complete.\")\n",
    "    print(\"\\nFeatures (X) verification:\")\n",
    "    print(f\"  Shape: {X.shape}\") # Expect (1000000, 10)\n",
    "    if target in X.columns:\n",
    "         print(f\"  ERROR: Target column '{target}' still present in X!\")\n",
    "    else:\n",
    "         print(f\"  Target column '{target}' successfully removed from X.\")\n",
    "    print(\"  First 5 rows of X:\")\n",
    "    print(X.head())\n",
    "    print(\"\\nTarget (y) verification:\")\n",
    "    print(f\"  Shape: {y.shape}\") # Expect (1000000,)\n",
    "    print(f\"  Data type: {y.dtype}\") # Expect int64\n",
    "    print(\"  First 5 values of y:\")\n",
    "    print(y.head())\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"ERROR: Could not find target column '{target}' in df_processed.\")\n",
    "    print(f\"Available columns are: {list(df_processed.columns)}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during X/y separation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b09d9f-4a14-4dcf-a599-4bf711d59c10",
   "metadata": {},
   "source": [
    "Excellent! That output is perfect and confirms you have successfully completed Step 2.3: Separate Features (X) and Target (y).\n",
    "\n",
    "X now has the correct shape (1M rows, 10 feature columns).\n",
    "\n",
    "y now has the correct shape (1M rows, 1 target column as a Series) and data type (int64).\n",
    "\n",
    "The verification checks confirm the target was correctly removed from X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ecc5ca-07ae-4f5d-be92-94edf00c3ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.3: Train/Test Split ---\n",
      "Data split into training and testing sets.\n",
      "X_train shape: (800000, 10), y_train shape: (800000,)\n",
      "X_test shape: (200000, 10), y_test shape: (200000,)\n",
      "\n",
      "Training set 'isFraud' distribution:\n",
      "isFraud\n",
      "0    0.998669\n",
      "1    0.001331\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set 'isFraud' distribution:\n",
      "isFraud\n",
      "0    0.99867\n",
      "1    0.00133\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-step 1.3 - Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n--- Step 1.3: Train/Test Split ---\")\n",
    "\n",
    "\n",
    "# Ensure X and y exist and are not empty\n",
    "if not 'X' in locals() or X.empty or not 'y' in locals() or y.empty:\n",
    "     print(\"ERROR: Features (X) or target (y) not defined or empty.\")\n",
    "     raise NameError(\"X or y not defined/empty.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=50,\n",
    "        stratify=y # Preserve class distribution\n",
    "    )\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(\"\\nTraining set 'isFraud' distribution:\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print(\"\\nTest set 'isFraud' distribution:\")\n",
    "    print(y_test.value_counts(normalize=True))\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during train/test split: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbb3ede-58f6-4ab7-b486-297caacda3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da04bd06-45a5-4458-bbdd-5c65b3c7b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.4: Apply SMOTE to Training Data ---\n",
      "Original training shape: X=(800000, 10), y=(800000,)\n",
      "Original training class distribution:\n",
      "isFraud\n",
      "0    0.998669\n",
      "1    0.001331\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Applying SMOTE (this might take a moment)...\n",
      "SMOTE application complete.\n",
      "Resampled training shape: X=(1597870, 10), y=(1597870,)\n",
      "\n",
      "Resampled training class distribution:\n",
      "isFraud\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.4: Handle Class Imbalance (SMOTE)\n",
    "from imblearn.over_sampling import SMOTE # Make sure imblearn is installed\n",
    "print(\"\\n--- Step 1.4: Apply SMOTE to Training Data ---\")\n",
    "\n",
    "\n",
    "# Ensure X_train and y_train exist\n",
    "if not 'X_train' in locals() or not 'y_train' in locals():\n",
    "     print(\"ERROR: X_train or y_train not defined.\")\n",
    "     raise NameError(\"X_train/y_train missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Instantiate SMOTE - random_state for reproducibility, n_jobs to speed up if possible\n",
    "    smote = SMOTE(random_state=50)\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Original training class distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "    print(\"\\nApplying SMOTE (this might take a moment)...\")\n",
    "    # Fit SMOTE and resample ONLY the training data\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(\"SMOTE application complete.\")\n",
    "    print(f\"Resampled training shape: X={X_train_resampled.shape}, y={y_train_resampled.shape}\")\n",
    "    print(f\"\\nResampled training class distribution:\\n{y_train_resampled.value_counts(normalize=True)}\") # Should be balanced\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during SMOTE: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f645ff-4a3b-4ea6-bbb4-8226939f80bc",
   "metadata": {},
   "source": [
    "EXPLANATION ON 1.4\n",
    "Original training shape: X=(800000, 10), y=(800000,)\n",
    "This shows the size of your training dataset before applying SMOTE.\n",
    "You had 800,000 samples (rows).\n",
    "X_train had 10 feature columns.\n",
    "y_train had 800,000 corresponding labels.\n",
    "Original training class distribution:\n",
    "isFraud\n",
    "0 0.999465: Roughly 99.95% of your original training data was labelled 'Not Fraud' (class 0).\n",
    "1 0.000535: Only about 0.05% of your original training data was labelled 'Fraud' (class 1).\n",
    "This confirms the severe class imbalance that SMOTE is designed to address. The model would likely ignore the tiny 'Fraud' class if trained on this original data.\n",
    "Applying SMOTE (this might take a moment)...\n",
    "This indicates that the smote.fit_resample(X_train, y_train) command started executing. Since n_jobs was removed, it likely ran on a single CPU core.\n",
    "SMOTE application complete.\n",
    "Confirmation that the process finished without errors.\n",
    "Resampled training shape: X=(1599144, 10), y=(1599144,)\n",
    "This shows the size of your training dataset after applying SMOTE (X_train_resampled, y_train_resampled).\n",
    "The number of samples has significantly increased to 1,599,144.\n",
    "Why the increase? SMOTE works by oversampling the minority class. It doesn't remove majority samples. It creates new, synthetic samples for the minority ('Fraud') class until the number of minority samples equals the number of majority samples.\n",
    "The number of features (10) remains the same, as SMOTE only creates new samples (rows), not new features.\n",
    "Resampled training class distribution:\n",
    "isFraud\n",
    "0 0.5: Exactly 50% of the resampled training data is now 'Not Fraud'.\n",
    "1 0.5: Exactly 50% of the resampled training data is now 'Fraud'.\n",
    "This confirms the successful outcome of SMOTE: the training dataset is now perfectly balanced.\n",
    "In essence: SMOTE successfully addressed the class imbalance by generating synthetic 'Fraud' examples, resulting in a larger, balanced training dataset (X_train_resampled, y_train_resampled). This balanced dataset will now be used to train the XGBoost model in the next step, forcing the model to pay equal attention to both 'Fraud' and 'Not Fraud' patterns, which is crucial for achieving good Recall on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d9185f-8cf7-4bce-b144-12eec7d181fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.5: Train Baseline XGBoost Model ---\n",
      "XGBoost Classifier instantiated.\n",
      "Training XGBoost model on resampled data (takes time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:15:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model training complete.\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.5: Train Baseline Model (XGBoost)\n",
    "import xgboost as xgb\n",
    "print(\"\\n--- Step 1.5: Train Baseline XGBoost Model ---\")\n",
    "\n",
    "\n",
    "# Ensure resampled training data exists\n",
    "if not 'X_train_resampled' in locals() or not 'y_train_resampled' in locals():\n",
    "     print(\"ERROR: Resampled training data not found.\")\n",
    "     raise NameError(\"Resampled data missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Instantiate the classifier\n",
    "    model_xgb_baseline = xgb.XGBClassifier(\n",
    "        random_state=50,          # For reproducibility of internal randomness\n",
    "        use_label_encoder=False,  # Avoids potential deprecation warnings\n",
    "        eval_metric='logloss'     # Common metric, avoids potential warnings if not set\n",
    "    )\n",
    "    print(\"XGBoost Classifier instantiated.\")\n",
    "    print(\"Training XGBoost model on resampled data (takes time)...\")\n",
    "\n",
    "\n",
    "    # Train the model using the balanced (SMOTE'd) training data\n",
    "    model_xgb_baseline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "    print(\"Baseline model training complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during XGBoost training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfe72d-2b45-4d0a-86fb-db2e0d3df5d2",
   "metadata": {},
   "source": [
    "--- Step 1.5: Train Baseline XGBoost Model ---: Indicates the start of this step.\n",
    "XGBoost Classifier instantiated.: Confirms that the xgb.XGBClassifier(...) line executed correctly, creating the model object before training.\n",
    "Training XGBoost model on resampled data (takes time)...: Shows that the model_xgb_baseline.fit(X_train_resampled, y_train_resampled) command started. This is where the actual learning happens, using the SMOTE-balanced data.\n",
    "UserWarning: [...] Parameters: { \"use_label_encoder\" } are not used.:\n",
    "What it means: XGBoost is telling you that even though you provided the parameter use_label_encoder=False when creating the classifier, this parameter is essentially ignored or not needed by the underlying training mechanism in newer versions, especially when you also specify eval_metric.\n",
    "Why it happens: use_label_encoder was primarily relevant for older versions or specific internal label handling that is now deprecated or automatically handled differently. Setting it to False is the recommended practice to avoid potential future issues, but the library is just letting you know it didn't actively use that specific setting during this particular training run.\n",
    "Is it a problem? No. This is a common warning and can be safely ignored. It does not mean the training failed or is incorrect.\n",
    "Baseline model training complete.: This is the most important message! It confirms that the .fit() process finished successfully without crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a70fc8-8494-4adc-80bb-687e0effa9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.6: Evaluate Baseline Model on Test Set ---\n",
      "Making predictions on the original (imbalanced) test set...\n",
      "\n",
      "Confusion Matrix (Baseline):\n",
      "[[199306    428]\n",
      " [     5    261]]\n",
      "\n",
      "Classification Report (Baseline):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Fraud (0)     1.0000    0.9979    0.9989    199734\n",
      "    Fraud (1)     0.3788    0.9812    0.5466       266\n",
      "\n",
      "     accuracy                         0.9978    200000\n",
      "    macro avg     0.6894    0.9895    0.7728    200000\n",
      " weighted avg     0.9991    0.9978    0.9983    200000\n",
      "\n",
      "\n",
      "Recall for Fraud Class (1): 0.9812\n",
      "\n",
      "Checking against MVP Recall Target (0.75)...\n",
      ">>> Level 1 MVP Recall target MET! :) <<<\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.6: Evaluate Baseline Model\n",
    "from sklearn.metrics import classification_report, recall_score, confusion_matrix\n",
    "print(\"\\n--- Step 1.6: Evaluate Baseline Model on Test Set ---\")\n",
    "\n",
    "\n",
    "# Ensure model and test data exist\n",
    "if not 'model_xgb_baseline' in locals(): raise NameError(\"Baseline model not trained.\")\n",
    "if not 'X_test' in locals() or not 'y_test' in locals(): raise NameError(\"Test data not available.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Making predictions on the original (imbalanced) test set...\")\n",
    "    # Use the trained model to predict on the unseen X_test\n",
    "    y_pred_baseline = model_xgb_baseline.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Baseline):\")\n",
    "    # Rows = Actual (0: Not Fraud, 1: Fraud)\n",
    "    # Cols = Predicted (0: Not Fraud, 1: Fraud)\n",
    "    cm = confusion_matrix(y_test, y_pred_baseline)\n",
    "    print(cm)\n",
    "    # You can manually interpret: TN=cm[0,0], FP=cm[0,1], FN=cm[1,0], TP=cm[1,1]\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report (Baseline):\")\n",
    "    print(classification_report(y_test, y_pred_baseline, target_names=['Not Fraud (0)', 'Fraud (1)'], digits=4))\n",
    "\n",
    "\n",
    "    # Calculate recall specifically for the positive 'Fraud' class (label=1)\n",
    "    recall_fraud = recall_score(y_test, y_pred_baseline, pos_label=1)\n",
    "    print(f\"\\nRecall for Fraud Class (1): {recall_fraud:.4f}\")\n",
    "\n",
    "\n",
    "    # Check MVP goal\n",
    "    MVP_RECALL_TARGET = 0.75 # Our defined target\n",
    "    print(f\"\\nChecking against MVP Recall Target ({MVP_RECALL_TARGET})...\")\n",
    "    if recall_fraud >= MVP_RECALL_TARGET:\n",
    "        print(f\">>> Level 1 MVP Recall target MET! :) <<<\")\n",
    "    else:\n",
    "        print(f\">>> Level 1 MVP Recall target NOT MET. :( <<<\")\n",
    "        print(f\"    (Further tuning in Level 2 or revisiting preprocessing might be needed)\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during baseline model evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8dc19-fcaa-453e-9bdb-dc2dd2f35b7e",
   "metadata": {},
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef243146-29b4-47c6-aa97-b69d9bd0404e",
   "metadata": {},
   "source": [
    "--- LEVEL 2 (Refined Model) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feb8177c-4fa5-4c6a-929c-093b6c96400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.1: Feature Engineering (Example) ---\n",
      "Original X_train shape: (800000, 10)\n",
      "Added 'amt_ratio_orig' feature.\n",
      "X_train_fe shape after FE: (800000, 11)\n",
      "X_test_fe shape after FE: (200000, 11)\n",
      "\n",
      "First 3 rows of X_train_fe with new feature(s):\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "6279968   644  267118.24       14636.00            0.00            0.00   \n",
      "4025611   299  415330.55           0.00            0.00       519118.41   \n",
      "67792       9  126211.60     5087050.38      5213261.98       327384.65   \n",
      "\n",
      "         newbalanceDest  type_CASH_OUT  type_DEBIT  type_PAYMENT  \\\n",
      "6279968       267118.24           True       False         False   \n",
      "4025611       934448.96           True       False         False   \n",
      "67792          49011.88          False       False         False   \n",
      "\n",
      "         type_TRANSFER  amt_ratio_orig  \n",
      "6279968          False    1.825077e+01  \n",
      "4025611          False    4.153306e+11  \n",
      "67792            False    2.481037e-02  \n",
      "\n",
      "Re-applying SMOTE to feature-engineered training data...\n",
      "SMOTE application complete on feature-engineered data.\n",
      "X_train_fe_resampled shape: (1597870, 11)\n",
      "y_train_fe_resampled shape: (1597870,)\n",
      "\n",
      "Resampled class distribution:\n",
      "isFraud\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.1: Feature Engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE # Re-import if in new notebook\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2.1: Feature Engineering (Example) ---\")\n",
    "\n",
    "\n",
    "# Ensure original train/test splits exist from Level 1\n",
    "if not 'X_train' in locals() or not 'X_test' in locals() or not 'y_train' in locals():\n",
    "     print(\"ERROR: Original train/test splits (X_train, X_test, y_train) not found.\")\n",
    "     raise NameError(\"Original splits missing.\")\n",
    "\n",
    "\n",
    "# Create copies to avoid modifying original dataframes\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe = X_test.copy()\n",
    "print(f\"Original X_train shape: {X_train_fe.shape}\")\n",
    "\n",
    "\n",
    "# Example 1: Amount / Origin Balance Ratio\n",
    "epsilon = 1e-6 # Small value to prevent division by zero\n",
    "X_train_fe['amt_ratio_orig'] = (X_train_fe['amount'] / (X_train_fe['oldbalanceOrg'] + epsilon)).fillna(0)\n",
    "X_test_fe['amt_ratio_orig'] = (X_test_fe['amount'] / (X_test_fe['oldbalanceOrg'] + epsilon)).fillna(0)\n",
    "print(\"Added 'amt_ratio_orig' feature.\")\n",
    "\n",
    "\n",
    "# Example 2: Balance Discrepancy (Optional - can add complexity)\n",
    "# X_train_fe['orig_bal_discrepancy'] = (X_train_fe['oldbalanceOrg'] - X_train_fe['newbalanceOrig']) - X_train_fe['amount']\n",
    "# X_test_fe['orig_bal_discrepancy'] = (X_test_fe['oldbalanceOrg'] - X_test_fe['newbalanceOrig']) - X_test_fe['amount']\n",
    "# print(\"Added 'orig_bal_discrepancy' feature.\")\n",
    "\n",
    "\n",
    "print(f\"X_train_fe shape after FE: {X_train_fe.shape}\")\n",
    "print(f\"X_test_fe shape after FE: {X_test_fe.shape}\")\n",
    "print(\"\\nFirst 3 rows of X_train_fe with new feature(s):\")\n",
    "print(X_train_fe.head(3))\n",
    "\n",
    "\n",
    "# --- Re-apply SMOTE on the NEW feature-engineered training data ---\n",
    "# We need to balance the training set *after* adding features\n",
    "print(\"\\nRe-applying SMOTE to feature-engineered training data...\")\n",
    "try:\n",
    "    smote_fe = SMOTE(random_state=50)\n",
    "    # Use the feature-engineered training data (X_train_fe) and original y_train\n",
    "    X_train_fe_resampled, y_train_fe_resampled = smote_fe.fit_resample(X_train_fe, y_train)\n",
    "    print(\"SMOTE application complete on feature-engineered data.\")\n",
    "    print(f\"X_train_fe_resampled shape: {X_train_fe_resampled.shape}\")\n",
    "    print(f\"y_train_fe_resampled shape: {y_train_fe_resampled.shape}\")\n",
    "    print(f\"\\nResampled class distribution:\\n{y_train_fe_resampled.value_counts(normalize=True)}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during SMOTE on feature-engineered data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579e5749-01a1-4782-b6aa-21226dfb54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.2: Build Scikit-learn Pipeline ---\n",
      "Pipeline object created successfully:\n",
      "Pipeline(steps=[('xgboost',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric='logloss',\n",
      "                               feature_types=None, feature_weights=None,\n",
      "                               gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.2: Build Scikit-learn Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler # Optional: if you wanted to add scaling\n",
    "import xgboost as xgb # Re-import if needed\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2.2: Build Scikit-learn Pipeline ---\")\n",
    "\n",
    "\n",
    "# Define the steps for the pipeline\n",
    "pipeline_steps = [\n",
    "    # Optional Step: Scaling (StandardScaler makes features have zero mean, unit variance)\n",
    "    # Though XGBoost is not sensitive to scaling, it's good practice if comparing models\n",
    "    # ('scaler', StandardScaler()),\n",
    "\n",
    "\n",
    "    # Final Step: The XGBoost Classifier\n",
    "    # Use the step name 'xgboost' to match parameter names later in tuning\n",
    "    ('xgboost', xgb.XGBClassifier(random_state=50, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "\n",
    "\n",
    "# Create the Pipeline object\n",
    "pipeline_obj = Pipeline(steps=pipeline_steps)\n",
    "\n",
    "\n",
    "print(\"Pipeline object created successfully:\")\n",
    "print(pipeline_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3eb35c7-705f-4ed4-8e83-43cd92cc0a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: hyperopt in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: ray[tune] in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (2.45.0)\n",
      "Requirement already satisfied: click>=7.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (8.1.8)\n",
      "Requirement already satisfied: filelock in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (3.18.0)\n",
      "Requirement already satisfied: jsonschema in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (1.1.0)\n",
      "Requirement already satisfied: packaging in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (24.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (6.30.2)\n",
      "Requirement already satisfied: pyyaml in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (6.0.2)\n",
      "Requirement already satisfied: requests in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2.32.3)\n",
      "Requirement already satisfied: pandas in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2.2.3)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (19.0.1)\n",
      "Requirement already satisfied: fsspec in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2024.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (25.1.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (1.6.1)\n",
      "Requirement already satisfied: six in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (1.17.0)\n",
      "Requirement already satisfied: networkx>=2.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (3.4.2)\n",
      "Requirement already satisfied: future in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: tqdm in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (3.1.1)\n",
      "Requirement already satisfied: py4j in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (0.10.9.9)\n",
      "Requirement already satisfied: colorama in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from click>=7.0->ray[tune]) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from referencing>=0.28.4->jsonschema->ray[tune]) (4.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from pandas->ray[tune]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from pandas->ray[tune]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from pandas->ray[tune]) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ray[tune] scikit-optimize hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95145a4-6b35-4d9f-8080-956da6528aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.3: Hyperparameter Tuning with RandomizedSearchCV ---\n",
      "Setting up RandomizedSearchCV with 15 iterations...\n",
      "Starting RandomizedSearch search (THIS CAN TAKE SIGNIFICANT TIME)...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [02:28:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomizedSearch search finished.\n",
      "Best parameters found: {'xgboost__colsample_bytree': np.float64(0.9157080755356058), 'xgboost__learning_rate': np.float64(0.08288386811018128), 'xgboost__max_depth': 8, 'xgboost__n_estimators': 472, 'xgboost__subsample': np.float64(0.8673350275106719)}\n",
      "Best cross-validation recall score during search: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFIED Sub-Step 2.3: Hyperparameter Tuning with RandomizedSearchCV ---\n",
    "# from ray.tune.integration.sklearn import TuneSearchCV # REMOVED RAY IMPORT\n",
    "from sklearn.model_selection import RandomizedSearchCV # USE SKLEARN'S TOOL\n",
    "from scipy.stats import randint, uniform\n",
    "# import ray is removed and replaced with sklearn's RandomizedSearchCV\n",
    "\n",
    "print(\"\\n--- Step 2.3: Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
    "\n",
    "# --- Ensure pipeline_obj exists from Step 2.2 ---\n",
    "if not 'pipeline_obj' in locals():\n",
    "     print(\"ERROR: 'pipeline_obj' (Pipeline object from Step 2.2) not found.\")\n",
    "     raise NameError(\"'pipeline_obj' is missing.\")\n",
    "\n",
    "# --- Define parameter search space (SAME AS BEFORE) ---\n",
    "# Prefix parameters with step name ('xgboost__')\n",
    "param_distributions = {\n",
    "    'xgboost__n_estimators': randint(150, 500),        # Number of trees\n",
    "    'xgboost__max_depth': randint(4, 12),             # Max depth of trees\n",
    "    'xgboost__learning_rate': uniform(0.01, 0.2),    # Step size shrinkage (range 0.01 to 0.21)\n",
    "    'xgboost__subsample': uniform(0.6, 0.4),         # Fraction of samples used per tree (range 0.6 to 1.0)\n",
    "    'xgboost__colsample_bytree': uniform(0.6, 0.4),  # Fraction of features used per tree (range 0.6 to 1.0)\n",
    "}\n",
    "\n",
    "# --- Setup RandomizedSearchCV (INSTEAD OF TuneSearchCV) ---\n",
    "n_iterations = 15 # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "print(f\"Setting up RandomizedSearchCV with {n_iterations} iterations...\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline_obj,              # The pipeline object\n",
    "    param_distributions=param_distributions, # The distributions to sample from\n",
    "    n_iter=n_iterations,       # Number of combinations to try (like n_trials)\n",
    "    scoring='recall',          # Metric to optimize for (use 'recall_macro' or specific scorer if needed)\n",
    "    cv=3,                      # 3-fold cross-validation\n",
    "    random_state=50,           # For reproducibility (use 42 if preferred from plan)\n",
    "    n_jobs=-1,                 # Use all available CPU cores for CV folds\n",
    "    verbose=2                  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Starting RandomizedSearch search (THIS CAN TAKE SIGNIFICANT TIME)...\")\n",
    "# Ensure feature-engineered, resampled training data exists from Step 2.1\n",
    "if not 'X_train_fe_resampled' in locals() or not 'y_train_fe_resampled' in locals():\n",
    "     print(\"ERROR: Feature-engineered resampled training data not found.\")\n",
    "     raise NameError(\"Feature-engineered resampled data missing.\")\n",
    "\n",
    "try:\n",
    "    # Fit RandomizedSearchCV on the FEATURE-ENGINEERED, SMOTE-RESAMPLED training data\n",
    "    random_search.fit(X_train_fe_resampled, y_train_fe_resampled)\n",
    "\n",
    "    print(\"\\nRandomizedSearch search finished.\")\n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "    print(f\"Best cross-validation recall score during search: {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # Access the best pipeline refitted on the whole training data\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during RandomizedSearch search: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ed826-9a84-4fea-9b69-4abc7d6b1592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.4: Evaluate Tuned Model on Test Set ---\n",
      "Making predictions with the tuned pipeline on the feature-engineered test set...\n",
      "\n",
      "Confusion Matrix (Tuned):\n",
      "[[199653     81]\n",
      " [     5    261]]\n",
      "\n",
      "Classification Report (Tuned):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Fraud (0)     1.0000    0.9996    0.9998    199734\n",
      "    Fraud (1)     0.7632    0.9812    0.8586       266\n",
      "\n",
      "     accuracy                         0.9996    200000\n",
      "    macro avg     0.8816    0.9904    0.9292    200000\n",
      " weighted avg     0.9997    0.9996    0.9996    200000\n",
      "\n",
      "\n",
      "Recall for Fraud Class (1) - Tuned: 0.9812\n",
      "(Recall Baseline was: 0.9812)\n",
      ">>> Recall did NOT improve vs Baseline.\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.4: Evaluate Tuned Model\n",
    "from sklearn.metrics import classification_report, recall_score, confusion_matrix\n",
    "print(\"\\n--- Step 2.4: Evaluate Tuned Model on Test Set ---\")\n",
    "\n",
    "\n",
    "# Ensure best_pipeline and X_test_fe exist\n",
    "if not 'best_pipeline' in locals():\n",
    "     print(\"ERROR: 'best_pipeline' not found. Tuning may have failed.\")\n",
    "     raise NameError(\"'best_pipeline' not defined.\")\n",
    "if not 'X_test_fe' in locals() or not 'y_test' in locals():\n",
    "     print(\"ERROR: Feature-engineered test data (X_test_fe) or y_test not found.\")\n",
    "     raise NameError(\"Test data missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Making predictions with the tuned pipeline on the feature-engineered test set...\")\n",
    "    # Predict using the best pipeline on X_test_fe\n",
    "    y_pred_tuned = best_pipeline.predict(X_test_fe)\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Tuned):\")\n",
    "    print(confusion_matrix(y_test, y_pred_tuned)) # Compare against original y_test\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report (Tuned):\")\n",
    "    print(classification_report(y_test, y_pred_tuned, target_names=['Not Fraud (0)', 'Fraud (1)'], digits=4))\n",
    "\n",
    "\n",
    "    # Calculate recall for the positive 'Fraud' class\n",
    "    recall_fraud_tuned = recall_score(y_test, y_pred_tuned, pos_label=1)\n",
    "    print(f\"\\nRecall for Fraud Class (1) - Tuned: {recall_fraud_tuned:.4f}\")\n",
    "\n",
    "\n",
    "    # Optional: Compare with baseline recall if stored from L1\n",
    "    if 'recall_fraud' in locals():\n",
    "          print(f\"(Recall Baseline was: {recall_fraud:.4f})\")\n",
    "          if recall_fraud_tuned > recall_fraud: print(\">>> Recall IMPROVED vs Baseline.\")\n",
    "          else: print(\">>> Recall did NOT improve vs Baseline.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during tuned model evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90cd87-5c03-457c-a26e-46aa1302ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.5: Save Best Pipeline & Commit ---\n",
      "Saved best pipeline object to 'best_fraud_pipeline.joblib'\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.5: Save Artifacts & Commit Level 2 Completion\n",
    "\n",
    "import joblib # Or use pickle\n",
    "print(\"\\n--- Step 2.5: Save Best Pipeline & Commit ---\")\n",
    "\n",
    "\n",
    "pipeline_filename = \"best_fraud_pipeline.joblib\"\n",
    "if 'best_pipeline' in locals():\n",
    "     try:\n",
    "         joblib.dump(best_pipeline, pipeline_filename)\n",
    "         print(f\"Saved best pipeline object to '{pipeline_filename}'\")\n",
    "     except Exception as e:\n",
    "         print(f\"ERROR saving pipeline: {e}\")\n",
    "else:\n",
    "     print(\"WARNING: 'best_pipeline' object not found, cannot save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a809b6b-cf8c-41ab-a291-5c627d5f040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9fb7c-60a9-45ff-b78b-aadba8a7bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
