{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6bbe082-1614-4dac-997c-5cbb3ac3321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load full dataset from: hf://datasets/MatrixIA/FraudData/FraudData.csv\n",
      "Successfully loaded full dataset into Pandas DataFrame.\n",
      "Full DataFrame shape: (6362620, 11)\n",
      "Creating a sample of 1000000 rows for initial development...\n",
      "Sample DataFrame shape: (1000000, 11)\n",
      "\n",
      "First 5 rows (from sample):\n",
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155             0.0             0.0        0               0  \n",
      "1  M2044282225             0.0             0.0        0               0  \n",
      "2   C553264065             0.0             0.0        1               0  \n",
      "3    C38997010         21182.0             0.0        1               0  \n",
      "4  M1230701703             0.0             0.0        0               0  \n",
      "\n",
      "DataFrame Info (from sample):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   type            1000000 non-null  object \n",
      " 2   amount          1000000 non-null  float64\n",
      " 3   nameOrig        1000000 non-null  object \n",
      " 4   oldbalanceOrg   1000000 non-null  float64\n",
      " 5   newbalanceOrig  1000000 non-null  float64\n",
      " 6   nameDest        1000000 non-null  object \n",
      " 7   oldbalanceDest  1000000 non-null  float64\n",
      " 8   newbalanceDest  1000000 non-null  float64\n",
      " 9   isFraud         1000000 non-null  int64  \n",
      " 10  isFlaggedFraud  1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 83.9+ MB\n",
      "\n",
      "Summary Statistics (from sample):\n",
      "                 step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  1000000.000000  1.000000e+06   1.000000e+06    1.000000e+06   \n",
      "mean        25.156387  1.602499e+05   8.776703e+05    8.983465e+05   \n",
      "std         12.652100  2.592584e+05   2.982420e+06    3.019326e+06   \n",
      "min          1.000000  1.000000e-01   0.000000e+00    0.000000e+00   \n",
      "25%         14.000000  1.275993e+04   0.000000e+00    0.000000e+00   \n",
      "50%         20.000000  7.953670e+04   1.595700e+04    0.000000e+00   \n",
      "75%         38.000000  2.166060e+05   1.397520e+05    1.797911e+05   \n",
      "max         45.000000  1.000000e+07   3.893942e+07    3.894623e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest         isFraud  isFlaggedFraud  \n",
      "count    1.000000e+06    1.000000e+06  1000000.000000       1000000.0  \n",
      "mean     9.860668e+05    1.125662e+06        0.000535             0.0  \n",
      "std      2.305423e+06    2.426587e+06        0.023124             0.0  \n",
      "min      0.000000e+00    0.000000e+00        0.000000             0.0  \n",
      "25%      0.000000e+00    0.000000e+00        0.000000             0.0  \n",
      "50%      1.349287e+05    2.301105e+05        0.000000             0.0  \n",
      "75%      9.268256e+05    1.167926e+06        0.000000             0.0  \n",
      "max      4.205466e+07    4.216916e+07        1.000000             0.0  \n",
      "\n",
      "Missing Values Count per Column (from sample):\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution for Target Column ('isFraud') in Sample:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Raw Counts in Sample:\n",
      "isFraud\n",
      "0    999465\n",
      "1       535\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the special Hugging Face URL for the CSV file\n",
    "# This tells pandas to use fsspec and huggingface_hub to find and read the file\n",
    "hf_csv_url = \"hf://datasets/MatrixIA/FraudData/FraudData.csv\"\n",
    "\n",
    "print(f\"Attempting to load full dataset from: {hf_csv_url}\")\n",
    "try:\n",
    "    # Use pandas read_csv directly with the hf:// URL\n",
    "    # This loads the entire dataset into memory. May take a minute or two.\n",
    "    df_full = pd.read_csv(hf_csv_url)\n",
    "    print(\"Successfully loaded full dataset into Pandas DataFrame.\")\n",
    "    print(f\"Full DataFrame shape: {df_full.shape}\")\n",
    "\n",
    "    # --- IMPORTANT: Create a Sample for Development ---\n",
    "    # Define sample size (e.g., 1 million rows)\n",
    "    sample_size = 1000000\n",
    "    print(f\"Creating a sample of {sample_size} rows for initial development...\")\n",
    "\n",
    "    # Option 1: Take the first N rows (simplest)\n",
    "    df = df_full.head(sample_size).copy()\n",
    "\n",
    "    # Option 2: Take a random sample (better representation, might be slightly slower)\n",
    "    # df = df_full.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "    print(f\"Sample DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # Optional: Delete the full dataframe to free memory if you notice slowdowns\n",
    "    # Although with 24GB RAM, it might not be necessary yet.\n",
    "    # del df_full\n",
    "    # import gc # Garbage collector\n",
    "    # gc.collect() # Force memory cleanup\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch potential errors during download or reading\n",
    "    print(f\"ERROR: Failed to load dataset using pd.read_csv('hf://...'). Error: {e}\")\n",
    "    print(\"Check your internet connection, proxy settings (if any), and the URL.\")\n",
    "    raise # Stop execution\n",
    "\n",
    "# --- Basic Inspection (Run on the SAMPLE 'df') ---\n",
    "# This part remains the same as before, using the 'df' variable which now holds the sample\n",
    "print(\"\\nFirst 5 rows (from sample):\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataFrame Info (from sample):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary Statistics (from sample):\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values Count per Column (from sample):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# --- Target Variable Check (Run on the SAMPLE 'df') ---\n",
    "# Target column is 'isFraud' for this dataset (PaySim)\n",
    "target_column = 'isFraud'\n",
    "\n",
    "if target_column in df.columns:\n",
    "    print(f\"\\nClass Distribution for Target Column ('{target_column}') in Sample:\")\n",
    "    print(df[target_column].value_counts(normalize=True))\n",
    "    print(\"\\nRaw Counts in Sample:\")\n",
    "    print(df[target_column].value_counts(normalize=False))\n",
    "else:\n",
    "    print(f\"\\nERROR: Target column '{target_column}' not found in the DataFrame!\")\n",
    "    print(f\"Available columns are: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f27c075-4799-4c72-927f-9f182e208fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating data profiling report on the SAMPLE... (This might take a minute or two)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8107e199c94f0a83a3ed41bd769fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|███████▌                                                                           | 1/11 [00:03<00:30,  3.03s/it]\u001b[A\n",
      " 18%|███████████████                                                                    | 2/11 [00:04<00:17,  1.94s/it]\u001b[A\n",
      " 27%|██████████████████████▋                                                            | 3/11 [00:09<00:27,  3.42s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:11<00:00,  1.04s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15ef6863d3948bb959c79054bcdbaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5348c35c74f4a7eb2e5638ae875a9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7df9dfdf0e44dd90a275fd8c5f8036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling report saved to fraud_data_profiling_report_sample.html\n"
     ]
    }
   ],
   "source": [
    "# Import the profiling tool\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pandas_profiling import ProfileReport\n",
    "    except ImportError:\n",
    "        print(\"ERROR: Neither ydata-profiling nor pandas-profiling seem to be installed.\")\n",
    "        print(\"Please run: pip install ydata-profiling\")\n",
    "        raise # Stop if library isn't installed\n",
    "\n",
    "print(\"\\nGenerating data profiling report on the SAMPLE... (This might take a minute or two)\")\n",
    "\n",
    "# Create the report object using the SAMPLE DataFrame 'df'\n",
    "profile = ProfileReport(df, title=\"Fraud Data Profiling Report (Sample)\", explorative=True)\n",
    "\n",
    "# Define the filename for the HTML report\n",
    "report_filename = \"fraud_data_profiling_report_sample.html\"\n",
    "\n",
    "# Save the report to an HTML file\n",
    "profile.to_file(report_filename)\n",
    "\n",
    "print(f\"Profiling report saved to {report_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850fcbf0-0359-45d1-939e-94c97cf48915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Step 2: Basic Preprocessing ---\n",
      "Original sample shape: (1000000, 11)\n",
      "Keeping features: ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
      "Target variable: isFraud\n",
      "\n",
      "Created df_processed with selected columns.\n",
      "Shape after selecting features: (1000000, 8)\n",
      "First 5 rows of df_processed:\n",
      "   step      type    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   PAYMENT   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   PAYMENT   1864.28        21249.0        19384.72             0.0   \n",
      "2     1  TRANSFER    181.00          181.0            0.00             0.0   \n",
      "3     1  CASH_OUT    181.00          181.0            0.00         21182.0   \n",
      "4     1   PAYMENT  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  isFraud  \n",
      "0             0.0        0  \n",
      "1             0.0        0  \n",
      "2             0.0        1  \n",
      "3             0.0        1  \n",
      "4             0.0        0  \n"
     ]
    }
   ],
   "source": [
    "#Step 2 Level 1: Preprocessing-and-Baseline\n",
    "# --- Step 2: Basic Preprocessing ---\n",
    "print(\"--- Starting Step 2: Basic Preprocessing ---\")\n",
    "\n",
    "# --- Step 2.1: Feature Selection ---\n",
    "# First, ensure 'df' holds the sample DataFrame from Step 1\n",
    "# (If in a new notebook, you might need to re-load/re-sample or load from a saved file)\n",
    "print(f\"Original sample shape: {df.shape}\")\n",
    "\n",
    "# Define the features we decided to keep\n",
    "features_to_keep = ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "target = 'isFraud' # Define the target column name\n",
    "\n",
    "print(f\"Keeping features: {features_to_keep}\")\n",
    "print(f\"Target variable: {target}\")\n",
    "\n",
    "try:\n",
    "    # Create a new DataFrame containing only the selected features and the target\n",
    "    # Using .copy() prevents accidental changes to the original 'df'\n",
    "    df_processed = df[features_to_keep + [target]].copy()\n",
    "\n",
    "    print(\"\\nCreated df_processed with selected columns.\")\n",
    "    print(f\"Shape after selecting features: {df_processed.shape}\")\n",
    "    print(\"First 5 rows of df_processed:\")\n",
    "    print(df_processed.head())\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A specified column was not found in the DataFrame: {e}\")\n",
    "    print(\"Please check the 'features_to_keep' list and the 'target' variable against the columns from df.info().\")\n",
    "    raise # Stop execution if columns are incorrect\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during feature selection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641233c4-ff29-4aa5-a442-8455fdbfc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types before encoding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   type            1000000 non-null  object \n",
      " 2   amount          1000000 non-null  float64\n",
      " 3   oldbalanceOrg   1000000 non-null  float64\n",
      " 4   newbalanceOrig  1000000 non-null  float64\n",
      " 5   oldbalanceDest  1000000 non-null  float64\n",
      " 6   newbalanceDest  1000000 non-null  float64\n",
      " 7   isFraud         1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 61.0+ MB\n",
      "None\n",
      "\n",
      "Unique values in 'type' column:\n",
      "['PAYMENT' 'TRANSFER' 'CASH_OUT' 'DEBIT' 'CASH_IN']\n",
      "\n",
      "Value counts for 'type':\n",
      "type\n",
      "CASH_OUT    362676\n",
      "PAYMENT     329753\n",
      "CASH_IN     218673\n",
      "TRANSFER     82424\n",
      "DEBIT         6474\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying one-hot encoding to: ['type'] using pd.get_dummies...\n",
      "\n",
      "DataFrame after one-hot encoding:\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   1864.28        21249.0        19384.72             0.0   \n",
      "2     1    181.00          181.0            0.00             0.0   \n",
      "3     1    181.00          181.0            0.00         21182.0   \n",
      "4     1  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  isFraud  type_CASH_OUT  type_DEBIT  type_PAYMENT  \\\n",
      "0             0.0        0          False       False          True   \n",
      "1             0.0        0          False       False          True   \n",
      "2             0.0        1          False       False         False   \n",
      "3             0.0        1           True       False         False   \n",
      "4             0.0        0          False       False          True   \n",
      "\n",
      "   type_TRANSFER  \n",
      "0          False  \n",
      "1          False  \n",
      "2           True  \n",
      "3          False  \n",
      "4          False  \n",
      "\n",
      "Shape after encoding: (1000000, 11)\n",
      "\n",
      "Data types after encoding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   amount          1000000 non-null  float64\n",
      " 2   oldbalanceOrg   1000000 non-null  float64\n",
      " 3   newbalanceOrig  1000000 non-null  float64\n",
      " 4   oldbalanceDest  1000000 non-null  float64\n",
      " 5   newbalanceDest  1000000 non-null  float64\n",
      " 6   isFraud         1000000 non-null  int64  \n",
      " 7   type_CASH_OUT   1000000 non-null  bool   \n",
      " 8   type_DEBIT      1000000 non-null  bool   \n",
      " 9   type_PAYMENT    1000000 non-null  bool   \n",
      " 10  type_TRANSFER   1000000 non-null  bool   \n",
      "dtypes: bool(4), float64(5), int64(2)\n",
      "memory usage: 57.2 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2.2: Encode Categorical Features ('type') ---\n",
    "\n",
    "# Identify the categorical column(s) remaining in df_processed\n",
    "categorical_cols = ['type']\n",
    "\n",
    "# Check current data types and unique values in 'type' before encoding\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(df_processed.info()) # Check df_processed specifically\n",
    "if 'type' in df_processed.columns:\n",
    "    print(f\"\\nUnique values in 'type' column:\\n{df_processed['type'].unique()}\")\n",
    "    print(f\"\\nValue counts for 'type':\\n{df_processed['type'].value_counts()}\")\n",
    "else:\n",
    "    print(\"\\n'type' column not found in df_processed (already dropped or renamed?).\")\n",
    "\n",
    "\n",
    "print(f\"\\nApplying one-hot encoding to: {categorical_cols} using pd.get_dummies...\")\n",
    "\n",
    "try:\n",
    "    # Use pd.get_dummies to convert the 'type' column\n",
    "    # drop_first=True removes redundancy (prevents multicollinearity)\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True) # This line reassigns df_processed\n",
    "\n",
    "    print(\"\\nDataFrame after one-hot encoding:\")\n",
    "    print(df_processed.head()) # Notice the original 'type' column is gone\n",
    "                               # and new 'type_TRANSFER', 'type_PAYMENT', etc. columns appear\n",
    "    print(f\"\\nShape after encoding: {df_processed.shape}\")\n",
    "\n",
    "    # Verify that all columns (except potentially target) are now numerical\n",
    "    print(\"\\nData types after encoding:\")\n",
    "    df_processed.info()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Column specified for encoding not found: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during one-hot encoding: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f7f52a-842b-472e-8cab-05b1199a4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.2.3: Separate Features (X) and Target (y) ---\n",
      "\n",
      "Separating features (X) and target ('isFraud')...\n",
      "Separation complete.\n",
      "\n",
      "Features (X) verification:\n",
      "  Shape: (1000000, 10)\n",
      "  Target column 'isFraud' successfully removed from X.\n",
      "  First 5 rows of X:\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   1864.28        21249.0        19384.72             0.0   \n",
      "2     1    181.00          181.0            0.00             0.0   \n",
      "3     1    181.00          181.0            0.00         21182.0   \n",
      "4     1  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  type_CASH_OUT  type_DEBIT  type_PAYMENT  type_TRANSFER  \n",
      "0             0.0          False       False          True          False  \n",
      "1             0.0          False       False          True          False  \n",
      "2             0.0          False       False         False           True  \n",
      "3             0.0           True       False         False          False  \n",
      "4             0.0          False       False          True          False  \n",
      "\n",
      "Target (y) verification:\n",
      "  Shape: (1000000,)\n",
      "  Data type: int64\n",
      "  First 5 values of y:\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: isFraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1.2.3: Separate Features (X) and Target (y) ---\n",
    "\n",
    "# Ensure the target variable name is correctly defined\n",
    "target = 'isFraud'\n",
    "\n",
    "print(\"\\n--- Step 1.2.3: Separate Features (X) and Target (y) ---\")\n",
    "print(f\"\\nSeparating features (X) and target ('{target}')...\")\n",
    "\n",
    "try:\n",
    "    # IMPORTANT: Make sure 'df_processed' is the DataFrame from the previous step\n",
    "    # containing the one-hot encoded 'type' columns.\n",
    "\n",
    "    # Create the features DataFrame 'X' by dropping the target column\n",
    "    # axis=1 specifies we are dropping a column\n",
    "    X = df_processed.drop(target, axis=1) # X should have 10 columns\n",
    "\n",
    "    # Create the target Series 'y' by selecting only the target column\n",
    "    y = df_processed[target] # y should be a Series\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(\"Separation complete.\")\n",
    "    print(\"\\nFeatures (X) verification:\")\n",
    "    print(f\"  Shape: {X.shape}\") # Expect (1000000, 10)\n",
    "    if target in X.columns:\n",
    "         print(f\"  ERROR: Target column '{target}' still present in X!\")\n",
    "    else:\n",
    "         print(f\"  Target column '{target}' successfully removed from X.\")\n",
    "    print(\"  First 5 rows of X:\")\n",
    "    print(X.head())\n",
    "    print(\"\\nTarget (y) verification:\")\n",
    "    print(f\"  Shape: {y.shape}\") # Expect (1000000,)\n",
    "    print(f\"  Data type: {y.dtype}\") # Expect int64\n",
    "    print(\"  First 5 values of y:\")\n",
    "    print(y.head())\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"ERROR: Could not find target column '{target}' in df_processed.\")\n",
    "    print(f\"Available columns are: {list(df_processed.columns)}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during X/y separation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b09d9f-4a14-4dcf-a599-4bf711d59c10",
   "metadata": {},
   "source": [
    "Excellent! That output is perfect and confirms you have successfully completed Step 2.3: Separate Features (X) and Target (y).\n",
    "\n",
    "X now has the correct shape (1M rows, 10 feature columns).\n",
    "\n",
    "y now has the correct shape (1M rows, 1 target column as a Series) and data type (int64).\n",
    "\n",
    "The verification checks confirm the target was correctly removed from X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ecc5ca-07ae-4f5d-be92-94edf00c3ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.3: Train/Test Split ---\n",
      "Data split into training and testing sets.\n",
      "X_train shape: (800000, 10), y_train shape: (800000,)\n",
      "X_test shape: (200000, 10), y_test shape: (200000,)\n",
      "\n",
      "Training set 'isFraud' distribution:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set 'isFraud' distribution:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-step 1.3 - Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n--- Step 1.3: Train/Test Split ---\")\n",
    "\n",
    "\n",
    "# Ensure X and y exist and are not empty\n",
    "if not 'X' in locals() or X.empty or not 'y' in locals() or y.empty:\n",
    "     print(\"ERROR: Features (X) or target (y) not defined or empty.\")\n",
    "     raise NameError(\"X or y not defined/empty.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=50,\n",
    "        stratify=y # Preserve class distribution\n",
    "    )\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(\"\\nTraining set 'isFraud' distribution:\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print(\"\\nTest set 'isFraud' distribution:\")\n",
    "    print(y_test.value_counts(normalize=True))\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during train/test split: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fbb3ede-58f6-4ab7-b486-297caacda3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da04bd06-45a5-4458-bbdd-5c65b3c7b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.4: Apply SMOTE to Training Data ---\n",
      "Original training shape: X=(800000, 10), y=(800000,)\n",
      "Original training class distribution:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Applying SMOTE (this might take a moment)...\n",
      "SMOTE application complete.\n",
      "Resampled training shape: X=(1599144, 10), y=(1599144,)\n",
      "\n",
      "Resampled training class distribution:\n",
      "isFraud\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.4: Handle Class Imbalance (SMOTE)\n",
    "from imblearn.over_sampling import SMOTE # Make sure imblearn is installed\n",
    "print(\"\\n--- Step 1.4: Apply SMOTE to Training Data ---\")\n",
    "\n",
    "\n",
    "# Ensure X_train and y_train exist\n",
    "if not 'X_train' in locals() or not 'y_train' in locals():\n",
    "     print(\"ERROR: X_train or y_train not defined.\")\n",
    "     raise NameError(\"X_train/y_train missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Instantiate SMOTE - random_state for reproducibility, n_jobs to speed up if possible\n",
    "    smote = SMOTE(random_state=50)\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Original training class distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "    print(\"\\nApplying SMOTE (this might take a moment)...\")\n",
    "    # Fit SMOTE and resample ONLY the training data\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(\"SMOTE application complete.\")\n",
    "    print(f\"Resampled training shape: X={X_train_resampled.shape}, y={y_train_resampled.shape}\")\n",
    "    print(f\"\\nResampled training class distribution:\\n{y_train_resampled.value_counts(normalize=True)}\") # Should be balanced\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during SMOTE: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f645ff-4a3b-4ea6-bbb4-8226939f80bc",
   "metadata": {},
   "source": [
    "EXPLANATION ON 1.4\n",
    "Original training shape: X=(800000, 10), y=(800000,)\n",
    "This shows the size of your training dataset before applying SMOTE.\n",
    "You had 800,000 samples (rows).\n",
    "X_train had 10 feature columns.\n",
    "y_train had 800,000 corresponding labels.\n",
    "Original training class distribution:\n",
    "isFraud\n",
    "0 0.999465: Roughly 99.95% of your original training data was labelled 'Not Fraud' (class 0).\n",
    "1 0.000535: Only about 0.05% of your original training data was labelled 'Fraud' (class 1).\n",
    "This confirms the severe class imbalance that SMOTE is designed to address. The model would likely ignore the tiny 'Fraud' class if trained on this original data.\n",
    "Applying SMOTE (this might take a moment)...\n",
    "This indicates that the smote.fit_resample(X_train, y_train) command started executing. Since n_jobs was removed, it likely ran on a single CPU core.\n",
    "SMOTE application complete.\n",
    "Confirmation that the process finished without errors.\n",
    "Resampled training shape: X=(1599144, 10), y=(1599144,)\n",
    "This shows the size of your training dataset after applying SMOTE (X_train_resampled, y_train_resampled).\n",
    "The number of samples has significantly increased to 1,599,144.\n",
    "Why the increase? SMOTE works by oversampling the minority class. It doesn't remove majority samples. It creates new, synthetic samples for the minority ('Fraud') class until the number of minority samples equals the number of majority samples.\n",
    "The number of features (10) remains the same, as SMOTE only creates new samples (rows), not new features.\n",
    "Resampled training class distribution:\n",
    "isFraud\n",
    "0 0.5: Exactly 50% of the resampled training data is now 'Not Fraud'.\n",
    "1 0.5: Exactly 50% of the resampled training data is now 'Fraud'.\n",
    "This confirms the successful outcome of SMOTE: the training dataset is now perfectly balanced.\n",
    "In essence: SMOTE successfully addressed the class imbalance by generating synthetic 'Fraud' examples, resulting in a larger, balanced training dataset (X_train_resampled, y_train_resampled). This balanced dataset will now be used to train the XGBoost model in the next step, forcing the model to pay equal attention to both 'Fraud' and 'Not Fraud' patterns, which is crucial for achieving good Recall on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76d9185f-8cf7-4bce-b144-12eec7d181fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.5: Train Baseline XGBoost Model ---\n",
      "XGBoost Classifier instantiated.\n",
      "Training XGBoost model on resampled data (takes time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [07:13:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model training complete.\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.5: Train Baseline Model (XGBoost)\n",
    "import xgboost as xgb\n",
    "print(\"\\n--- Step 1.5: Train Baseline XGBoost Model ---\")\n",
    "\n",
    "\n",
    "# Ensure resampled training data exists\n",
    "if not 'X_train_resampled' in locals() or not 'y_train_resampled' in locals():\n",
    "     print(\"ERROR: Resampled training data not found.\")\n",
    "     raise NameError(\"Resampled data missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Instantiate the classifier\n",
    "    model_xgb_baseline = xgb.XGBClassifier(\n",
    "        random_state=42,          # For reproducibility of internal randomness\n",
    "        use_label_encoder=False,  # Avoids potential deprecation warnings\n",
    "        eval_metric='logloss'     # Common metric, avoids potential warnings if not set\n",
    "    )\n",
    "    print(\"XGBoost Classifier instantiated.\")\n",
    "    print(\"Training XGBoost model on resampled data (takes time)...\")\n",
    "\n",
    "\n",
    "    # Train the model using the balanced (SMOTE'd) training data\n",
    "    model_xgb_baseline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "    print(\"Baseline model training complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during XGBoost training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfe72d-2b45-4d0a-86fb-db2e0d3df5d2",
   "metadata": {},
   "source": [
    "--- Step 1.5: Train Baseline XGBoost Model ---: Indicates the start of this step.\n",
    "XGBoost Classifier instantiated.: Confirms that the xgb.XGBClassifier(...) line executed correctly, creating the model object before training.\n",
    "Training XGBoost model on resampled data (takes time)...: Shows that the model_xgb_baseline.fit(X_train_resampled, y_train_resampled) command started. This is where the actual learning happens, using the SMOTE-balanced data.\n",
    "UserWarning: [...] Parameters: { \"use_label_encoder\" } are not used.:\n",
    "What it means: XGBoost is telling you that even though you provided the parameter use_label_encoder=False when creating the classifier, this parameter is essentially ignored or not needed by the underlying training mechanism in newer versions, especially when you also specify eval_metric.\n",
    "Why it happens: use_label_encoder was primarily relevant for older versions or specific internal label handling that is now deprecated or automatically handled differently. Setting it to False is the recommended practice to avoid potential future issues, but the library is just letting you know it didn't actively use that specific setting during this particular training run.\n",
    "Is it a problem? No. This is a common warning and can be safely ignored. It does not mean the training failed or is incorrect.\n",
    "Baseline model training complete.: This is the most important message! It confirms that the .fit() process finished successfully without crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7a70fc8-8494-4adc-80bb-687e0effa9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.6: Evaluate Baseline Model on Test Set ---\n",
      "Making predictions on the original (imbalanced) test set...\n",
      "\n",
      "Confusion Matrix (Baseline):\n",
      "[[199141    752]\n",
      " [    13     94]]\n",
      "\n",
      "Classification Report (Baseline):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Fraud (0)     0.9999    0.9962    0.9981    199893\n",
      "    Fraud (1)     0.1111    0.8785    0.1973       107\n",
      "\n",
      "     accuracy                         0.9962    200000\n",
      "    macro avg     0.5555    0.9374    0.5977    200000\n",
      " weighted avg     0.9995    0.9962    0.9977    200000\n",
      "\n",
      "\n",
      "Recall for Fraud Class (1): 0.8785\n",
      "\n",
      "Checking against MVP Recall Target (0.75)...\n",
      ">>> Level 1 MVP Recall target MET! :) <<<\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.6: Evaluate Baseline Model\n",
    "from sklearn.metrics import classification_report, recall_score, confusion_matrix\n",
    "print(\"\\n--- Step 1.6: Evaluate Baseline Model on Test Set ---\")\n",
    "\n",
    "\n",
    "# Ensure model and test data exist\n",
    "if not 'model_xgb_baseline' in locals(): raise NameError(\"Baseline model not trained.\")\n",
    "if not 'X_test' in locals() or not 'y_test' in locals(): raise NameError(\"Test data not available.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Making predictions on the original (imbalanced) test set...\")\n",
    "    # Use the trained model to predict on the unseen X_test\n",
    "    y_pred_baseline = model_xgb_baseline.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Baseline):\")\n",
    "    # Rows = Actual (0: Not Fraud, 1: Fraud)\n",
    "    # Cols = Predicted (0: Not Fraud, 1: Fraud)\n",
    "    cm = confusion_matrix(y_test, y_pred_baseline)\n",
    "    print(cm)\n",
    "    # You can manually interpret: TN=cm[0,0], FP=cm[0,1], FN=cm[1,0], TP=cm[1,1]\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report (Baseline):\")\n",
    "    print(classification_report(y_test, y_pred_baseline, target_names=['Not Fraud (0)', 'Fraud (1)'], digits=4))\n",
    "\n",
    "\n",
    "    # Calculate recall specifically for the positive 'Fraud' class (label=1)\n",
    "    recall_fraud = recall_score(y_test, y_pred_baseline, pos_label=1)\n",
    "    print(f\"\\nRecall for Fraud Class (1): {recall_fraud:.4f}\")\n",
    "\n",
    "\n",
    "    # Check MVP goal\n",
    "    MVP_RECALL_TARGET = 0.75 # Our defined target\n",
    "    print(f\"\\nChecking against MVP Recall Target ({MVP_RECALL_TARGET})...\")\n",
    "    if recall_fraud >= MVP_RECALL_TARGET:\n",
    "        print(f\">>> Level 1 MVP Recall target MET! :) <<<\")\n",
    "    else:\n",
    "        print(f\">>> Level 1 MVP Recall target NOT MET. :( <<<\")\n",
    "        print(f\"    (Further tuning in Level 2 or revisiting preprocessing might be needed)\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during baseline model evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8dc19-fcaa-453e-9bdb-dc2dd2f35b7e",
   "metadata": {},
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aae34a-13d1-4876-9a5e-90d6930c81e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection-env",
   "language": "python",
   "name": "fraud-detection-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
