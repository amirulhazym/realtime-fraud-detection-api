{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc64539-196b-4e67-b787-f78718efe6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: colorama in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247332bd-fcb3-49db-9495-11035a90ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load full dataset from: hf://datasets/MatrixIA/FraudData/FraudData.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9331ebc1-4021-401a-b701-138764da7a4f)')' thrown while requesting GET https://huggingface.co/datasets/MatrixIA/FraudData/resolve/main/FraudData.csv\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded full dataset into Pandas DataFrame.\n",
      "Full DataFrame shape: (6362620, 11)\n",
      "Creating a sample of 1000000 rows for initial development...\n",
      "Sample DataFrame shape: (1000000, 11)\n",
      "\n",
      "First 5 rows (from sample):\n",
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155             0.0             0.0        0               0  \n",
      "1  M2044282225             0.0             0.0        0               0  \n",
      "2   C553264065             0.0             0.0        1               0  \n",
      "3    C38997010         21182.0             0.0        1               0  \n",
      "4  M1230701703             0.0             0.0        0               0  \n",
      "\n",
      "DataFrame Info (from sample):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   type            1000000 non-null  object \n",
      " 2   amount          1000000 non-null  float64\n",
      " 3   nameOrig        1000000 non-null  object \n",
      " 4   oldbalanceOrg   1000000 non-null  float64\n",
      " 5   newbalanceOrig  1000000 non-null  float64\n",
      " 6   nameDest        1000000 non-null  object \n",
      " 7   oldbalanceDest  1000000 non-null  float64\n",
      " 8   newbalanceDest  1000000 non-null  float64\n",
      " 9   isFraud         1000000 non-null  int64  \n",
      " 10  isFlaggedFraud  1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 83.9+ MB\n",
      "\n",
      "Summary Statistics (from sample):\n",
      "                 step        amount  oldbalanceOrg  newbalanceOrig  \\\n",
      "count  1000000.000000  1.000000e+06   1.000000e+06    1.000000e+06   \n",
      "mean        25.156387  1.602499e+05   8.776703e+05    8.983465e+05   \n",
      "std         12.652100  2.592584e+05   2.982420e+06    3.019326e+06   \n",
      "min          1.000000  1.000000e-01   0.000000e+00    0.000000e+00   \n",
      "25%         14.000000  1.275993e+04   0.000000e+00    0.000000e+00   \n",
      "50%         20.000000  7.953670e+04   1.595700e+04    0.000000e+00   \n",
      "75%         38.000000  2.166060e+05   1.397520e+05    1.797911e+05   \n",
      "max         45.000000  1.000000e+07   3.893942e+07    3.894623e+07   \n",
      "\n",
      "       oldbalanceDest  newbalanceDest         isFraud  isFlaggedFraud  \n",
      "count    1.000000e+06    1.000000e+06  1000000.000000       1000000.0  \n",
      "mean     9.860668e+05    1.125662e+06        0.000535             0.0  \n",
      "std      2.305423e+06    2.426587e+06        0.023124             0.0  \n",
      "min      0.000000e+00    0.000000e+00        0.000000             0.0  \n",
      "25%      0.000000e+00    0.000000e+00        0.000000             0.0  \n",
      "50%      1.349287e+05    2.301105e+05        0.000000             0.0  \n",
      "75%      9.268256e+05    1.167926e+06        0.000000             0.0  \n",
      "max      4.205466e+07    4.216916e+07        1.000000             0.0  \n",
      "\n",
      "Missing Values Count per Column (from sample):\n",
      "step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "\n",
      "Class Distribution for Target Column ('isFraud') in Sample:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Raw Counts in Sample:\n",
      "isFraud\n",
      "0    999465\n",
      "1       535\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the special Hugging Face URL for the CSV file\n",
    "# This tells pandas to use fsspec and huggingface_hub to find and read the file\n",
    "hf_csv_url = \"hf://datasets/MatrixIA/FraudData/FraudData.csv\"\n",
    "\n",
    "print(f\"Attempting to load full dataset from: {hf_csv_url}\")\n",
    "try:\n",
    "    # Use pandas read_csv directly with the hf:// URL\n",
    "    # This loads the entire dataset into memory. May take a minute or two.\n",
    "    df_full = pd.read_csv(hf_csv_url)\n",
    "    print(\"Successfully loaded full dataset into Pandas DataFrame.\")\n",
    "    print(f\"Full DataFrame shape: {df_full.shape}\")\n",
    "\n",
    "    # --- IMPORTANT: Create a Sample for Development ---\n",
    "    # Define sample size (e.g., 1 million rows)\n",
    "    sample_size = 1000000\n",
    "    print(f\"Creating a sample of {sample_size} rows for initial development...\")\n",
    "\n",
    "    # Option 1: Take the first N rows (simplest)\n",
    "    df = df_full.head(sample_size).copy()\n",
    "\n",
    "    # Option 2: Take a random sample (better representation, might be slightly slower)\n",
    "    # df = df_full.sample(n=sample_size, random_state=42).copy()\n",
    "\n",
    "    print(f\"Sample DataFrame shape: {df.shape}\")\n",
    "\n",
    "    # Optional: Delete the full dataframe to free memory if you notice slowdowns\n",
    "    # Although with 24GB RAM, it might not be necessary yet.\n",
    "    # del df_full\n",
    "    # import gc # Garbage collector\n",
    "    # gc.collect() # Force memory cleanup\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch potential errors during download or reading\n",
    "    print(f\"ERROR: Failed to load dataset using pd.read_csv('hf://...'). Error: {e}\")\n",
    "    print(\"Check your internet connection, proxy settings (if any), and the URL.\")\n",
    "    raise # Stop execution\n",
    "\n",
    "# --- Basic Inspection (Run on the SAMPLE 'df') ---\n",
    "# This part remains the same as before, using the 'df' variable which now holds the sample\n",
    "print(\"\\nFirst 5 rows (from sample):\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataFrame Info (from sample):\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nSummary Statistics (from sample):\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values Count per Column (from sample):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# --- Target Variable Check (Run on the SAMPLE 'df') ---\n",
    "# Target column is 'isFraud' for this dataset (PaySim)\n",
    "target_column = 'isFraud'\n",
    "\n",
    "if target_column in df.columns:\n",
    "    print(f\"\\nClass Distribution for Target Column ('{target_column}') in Sample:\")\n",
    "    print(df[target_column].value_counts(normalize=True))\n",
    "    print(\"\\nRaw Counts in Sample:\")\n",
    "    print(df[target_column].value_counts(normalize=False))\n",
    "else:\n",
    "    print(f\"\\nERROR: Target column '{target_column}' not found in the DataFrame!\")\n",
    "    print(f\"Available columns are: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f27c075-4799-4c72-927f-9f182e208fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating data profiling report on the SAMPLE... (This might take a minute or two)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a1ddc4b44e4666a2fa39ad52a5baa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                         | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|█████▉                                                           | 1/11 [00:01<00:19,  1.90s/it]\u001b[A\n",
      " 27%|█████████████████▋                                               | 3/11 [00:08<00:24,  3.05s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████| 11/11 [00:09<00:00,  1.14it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcbcadef3da4c1bb883253c194a5883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9637975fbd0a40e2886985240da29fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cda07f77144d66b35d5755d1816f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling report saved to fraud_data_profiling_report_sample.html\n"
     ]
    }
   ],
   "source": [
    "# Import the profiling tool\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pandas_profiling import ProfileReport\n",
    "    except ImportError:\n",
    "        print(\"ERROR: Neither ydata-profiling nor pandas-profiling seem to be installed.\")\n",
    "        print(\"Please run: pip install ydata-profiling\")\n",
    "        raise # Stop if library isn't installed\n",
    "\n",
    "print(\"\\nGenerating data profiling report on the SAMPLE... (This might take a minute or two)\")\n",
    "\n",
    "# Create the report object using the SAMPLE DataFrame 'df'\n",
    "profile = ProfileReport(df, title=\"Fraud Data Profiling Report (Sample)\", explorative=True)\n",
    "\n",
    "# Define the filename for the HTML report\n",
    "report_filename = \"fraud_data_profiling_report_sample.html\"\n",
    "\n",
    "# Save the report to an HTML file\n",
    "profile.to_file(report_filename)\n",
    "\n",
    "print(f\"Profiling report saved to {report_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850fcbf0-0359-45d1-939e-94c97cf48915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Step 2: Basic Preprocessing ---\n",
      "Original sample shape: (1000000, 11)\n",
      "Keeping features: ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
      "Target variable: isFraud\n",
      "\n",
      "Created df_processed with selected columns.\n",
      "Shape after selecting features: (1000000, 8)\n",
      "First 5 rows of df_processed:\n",
      "   step      type    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   PAYMENT   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   PAYMENT   1864.28        21249.0        19384.72             0.0   \n",
      "2     1  TRANSFER    181.00          181.0            0.00             0.0   \n",
      "3     1  CASH_OUT    181.00          181.0            0.00         21182.0   \n",
      "4     1   PAYMENT  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  isFraud  \n",
      "0             0.0        0  \n",
      "1             0.0        0  \n",
      "2             0.0        1  \n",
      "3             0.0        1  \n",
      "4             0.0        0  \n"
     ]
    }
   ],
   "source": [
    "#Step 2 Level 1: Preprocessing-and-Baseline\n",
    "# --- Step 2: Basic Preprocessing ---\n",
    "print(\"--- Starting Step 2: Basic Preprocessing ---\")\n",
    "\n",
    "# --- Step 2.1: Feature Selection ---\n",
    "# First, ensure 'df' holds the sample DataFrame from Step 1\n",
    "# (If in a new notebook, you might need to re-load/re-sample or load from a saved file)\n",
    "print(f\"Original sample shape: {df.shape}\")\n",
    "\n",
    "# Define the features we decided to keep\n",
    "features_to_keep = ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "target = 'isFraud' # Define the target column name\n",
    "\n",
    "print(f\"Keeping features: {features_to_keep}\")\n",
    "print(f\"Target variable: {target}\")\n",
    "\n",
    "try:\n",
    "    # Create a new DataFrame containing only the selected features and the target\n",
    "    # Using .copy() prevents accidental changes to the original 'df'\n",
    "    df_processed = df[features_to_keep + [target]].copy()\n",
    "\n",
    "    print(\"\\nCreated df_processed with selected columns.\")\n",
    "    print(f\"Shape after selecting features: {df_processed.shape}\")\n",
    "    print(\"First 5 rows of df_processed:\")\n",
    "    print(df_processed.head())\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: A specified column was not found in the DataFrame: {e}\")\n",
    "    print(\"Please check the 'features_to_keep' list and the 'target' variable against the columns from df.info().\")\n",
    "    raise # Stop execution if columns are incorrect\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during feature selection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641233c4-ff29-4aa5-a442-8455fdbfc3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types before encoding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   type            1000000 non-null  object \n",
      " 2   amount          1000000 non-null  float64\n",
      " 3   oldbalanceOrg   1000000 non-null  float64\n",
      " 4   newbalanceOrig  1000000 non-null  float64\n",
      " 5   oldbalanceDest  1000000 non-null  float64\n",
      " 6   newbalanceDest  1000000 non-null  float64\n",
      " 7   isFraud         1000000 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 61.0+ MB\n",
      "None\n",
      "\n",
      "Unique values in 'type' column:\n",
      "['PAYMENT' 'TRANSFER' 'CASH_OUT' 'DEBIT' 'CASH_IN']\n",
      "\n",
      "Value counts for 'type':\n",
      "type\n",
      "CASH_OUT    362676\n",
      "PAYMENT     329753\n",
      "CASH_IN     218673\n",
      "TRANSFER     82424\n",
      "DEBIT         6474\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying one-hot encoding to: ['type'] using pd.get_dummies...\n",
      "\n",
      "DataFrame after one-hot encoding:\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   1864.28        21249.0        19384.72             0.0   \n",
      "2     1    181.00          181.0            0.00             0.0   \n",
      "3     1    181.00          181.0            0.00         21182.0   \n",
      "4     1  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  isFraud  type_CASH_OUT  type_DEBIT  type_PAYMENT  \\\n",
      "0             0.0        0          False       False          True   \n",
      "1             0.0        0          False       False          True   \n",
      "2             0.0        1          False       False         False   \n",
      "3             0.0        1           True       False         False   \n",
      "4             0.0        0          False       False          True   \n",
      "\n",
      "   type_TRANSFER  \n",
      "0          False  \n",
      "1          False  \n",
      "2           True  \n",
      "3          False  \n",
      "4          False  \n",
      "\n",
      "Shape after encoding: (1000000, 11)\n",
      "\n",
      "Data types after encoding:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count    Dtype  \n",
      "---  ------          --------------    -----  \n",
      " 0   step            1000000 non-null  int64  \n",
      " 1   amount          1000000 non-null  float64\n",
      " 2   oldbalanceOrg   1000000 non-null  float64\n",
      " 3   newbalanceOrig  1000000 non-null  float64\n",
      " 4   oldbalanceDest  1000000 non-null  float64\n",
      " 5   newbalanceDest  1000000 non-null  float64\n",
      " 6   isFraud         1000000 non-null  int64  \n",
      " 7   type_CASH_OUT   1000000 non-null  bool   \n",
      " 8   type_DEBIT      1000000 non-null  bool   \n",
      " 9   type_PAYMENT    1000000 non-null  bool   \n",
      " 10  type_TRANSFER   1000000 non-null  bool   \n",
      "dtypes: bool(4), float64(5), int64(2)\n",
      "memory usage: 57.2 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2.2: Encode Categorical Features ('type') ---\n",
    "\n",
    "# Identify the categorical column(s) remaining in df_processed\n",
    "categorical_cols = ['type']\n",
    "\n",
    "# Check current data types and unique values in 'type' before encoding\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(df_processed.info()) # Check df_processed specifically\n",
    "if 'type' in df_processed.columns:\n",
    "    print(f\"\\nUnique values in 'type' column:\\n{df_processed['type'].unique()}\")\n",
    "    print(f\"\\nValue counts for 'type':\\n{df_processed['type'].value_counts()}\")\n",
    "else:\n",
    "    print(\"\\n'type' column not found in df_processed (already dropped or renamed?).\")\n",
    "\n",
    "\n",
    "print(f\"\\nApplying one-hot encoding to: {categorical_cols} using pd.get_dummies...\")\n",
    "\n",
    "try:\n",
    "    # Use pd.get_dummies to convert the 'type' column\n",
    "    # drop_first=True removes redundancy (prevents multicollinearity)\n",
    "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True) # This line reassigns df_processed\n",
    "\n",
    "    print(\"\\nDataFrame after one-hot encoding:\")\n",
    "    print(df_processed.head()) # Notice the original 'type' column is gone\n",
    "                               # and new 'type_TRANSFER', 'type_PAYMENT', etc. columns appear\n",
    "    print(f\"\\nShape after encoding: {df_processed.shape}\")\n",
    "\n",
    "    # Verify that all columns (except potentially target) are now numerical\n",
    "    print(\"\\nData types after encoding:\")\n",
    "    df_processed.info()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Column specified for encoding not found: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during one-hot encoding: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f7f52a-842b-472e-8cab-05b1199a4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.2.3: Separate Features (X) and Target (y) ---\n",
      "\n",
      "Separating features (X) and target ('isFraud')...\n",
      "Separation complete.\n",
      "\n",
      "Features (X) verification:\n",
      "  Shape: (1000000, 10)\n",
      "  Target column 'isFraud' successfully removed from X.\n",
      "  First 5 rows of X:\n",
      "   step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0     1   9839.64       170136.0       160296.36             0.0   \n",
      "1     1   1864.28        21249.0        19384.72             0.0   \n",
      "2     1    181.00          181.0            0.00             0.0   \n",
      "3     1    181.00          181.0            0.00         21182.0   \n",
      "4     1  11668.14        41554.0        29885.86             0.0   \n",
      "\n",
      "   newbalanceDest  type_CASH_OUT  type_DEBIT  type_PAYMENT  type_TRANSFER  \n",
      "0             0.0          False       False          True          False  \n",
      "1             0.0          False       False          True          False  \n",
      "2             0.0          False       False         False           True  \n",
      "3             0.0           True       False         False          False  \n",
      "4             0.0          False       False          True          False  \n",
      "\n",
      "Target (y) verification:\n",
      "  Shape: (1000000,)\n",
      "  Data type: int64\n",
      "  First 5 values of y:\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: isFraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1.2.3: Separate Features (X) and Target (y) ---\n",
    "\n",
    "# Ensure the target variable name is correctly defined\n",
    "target = 'isFraud'\n",
    "\n",
    "print(\"\\n--- Step 1.2.3: Separate Features (X) and Target (y) ---\")\n",
    "print(f\"\\nSeparating features (X) and target ('{target}')...\")\n",
    "\n",
    "try:\n",
    "    # IMPORTANT: Make sure 'df_processed' is the DataFrame from the previous step\n",
    "    # containing the one-hot encoded 'type' columns.\n",
    "\n",
    "    # Create the features DataFrame 'X' by dropping the target column\n",
    "    # axis=1 specifies we are dropping a column\n",
    "    X = df_processed.drop(target, axis=1) # X should have 10 columns\n",
    "\n",
    "    # Create the target Series 'y' by selecting only the target column\n",
    "    y = df_processed[target] # y should be a Series\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(\"Separation complete.\")\n",
    "    print(\"\\nFeatures (X) verification:\")\n",
    "    print(f\"  Shape: {X.shape}\") # Expect (1000000, 10)\n",
    "    if target in X.columns:\n",
    "         print(f\"  ERROR: Target column '{target}' still present in X!\")\n",
    "    else:\n",
    "         print(f\"  Target column '{target}' successfully removed from X.\")\n",
    "    print(\"  First 5 rows of X:\")\n",
    "    print(X.head())\n",
    "    print(\"\\nTarget (y) verification:\")\n",
    "    print(f\"  Shape: {y.shape}\") # Expect (1000000,)\n",
    "    print(f\"  Data type: {y.dtype}\") # Expect int64\n",
    "    print(\"  First 5 values of y:\")\n",
    "    print(y.head())\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"ERROR: Could not find target column '{target}' in df_processed.\")\n",
    "    print(f\"Available columns are: {list(df_processed.columns)}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during X/y separation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b09d9f-4a14-4dcf-a599-4bf711d59c10",
   "metadata": {},
   "source": [
    "Excellent! That output is perfect and confirms you have successfully completed Step 2.3: Separate Features (X) and Target (y).\n",
    "\n",
    "X now has the correct shape (1M rows, 10 feature columns).\n",
    "\n",
    "y now has the correct shape (1M rows, 1 target column as a Series) and data type (int64).\n",
    "\n",
    "The verification checks confirm the target was correctly removed from X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ecc5ca-07ae-4f5d-be92-94edf00c3ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.3: Train/Test Split ---\n",
      "Data split into training and testing sets.\n",
      "X_train shape: (800000, 10), y_train shape: (800000,)\n",
      "X_test shape: (200000, 10), y_test shape: (200000,)\n",
      "\n",
      "Training set 'isFraud' distribution:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set 'isFraud' distribution:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-step 1.3 - Train/Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"\\n--- Step 1.3: Train/Test Split ---\")\n",
    "\n",
    "\n",
    "# Ensure X and y exist and are not empty\n",
    "if not 'X' in locals() or X.empty or not 'y' in locals() or y.empty:\n",
    "     print(\"ERROR: Features (X) or target (y) not defined or empty.\")\n",
    "     raise NameError(\"X or y not defined/empty.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=50,\n",
    "        stratify=y # Preserve class distribution\n",
    "    )\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(\"\\nTraining set 'isFraud' distribution:\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print(\"\\nTest set 'isFraud' distribution:\")\n",
    "    print(y_test.value_counts(normalize=True))\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during train/test split: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbb3ede-58f6-4ab7-b486-297caacda3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da04bd06-45a5-4458-bbdd-5c65b3c7b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.4: Apply SMOTE to Training Data ---\n",
      "Original training shape: X=(800000, 10), y=(800000,)\n",
      "Original training class distribution:\n",
      "isFraud\n",
      "0    0.999465\n",
      "1    0.000535\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Applying SMOTE (this might take a moment)...\n",
      "SMOTE application complete.\n",
      "Resampled training shape: X=(1599144, 10), y=(1599144,)\n",
      "\n",
      "Resampled training class distribution:\n",
      "isFraud\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.4: Handle Class Imbalance (SMOTE)\n",
    "from imblearn.over_sampling import SMOTE # Make sure imblearn is installed\n",
    "print(\"\\n--- Step 1.4: Apply SMOTE to Training Data ---\")\n",
    "\n",
    "\n",
    "# Ensure X_train and y_train exist\n",
    "if not 'X_train' in locals() or not 'y_train' in locals():\n",
    "     print(\"ERROR: X_train or y_train not defined.\")\n",
    "     raise NameError(\"X_train/y_train missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Instantiate SMOTE - random_state for reproducibility, n_jobs to speed up if possible\n",
    "    smote = SMOTE(random_state=50)\n",
    "    print(f\"Original training shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Original training class distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "    print(\"\\nApplying SMOTE (this might take a moment)...\")\n",
    "    # Fit SMOTE and resample ONLY the training data\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(\"SMOTE application complete.\")\n",
    "    print(f\"Resampled training shape: X={X_train_resampled.shape}, y={y_train_resampled.shape}\")\n",
    "    print(f\"\\nResampled training class distribution:\\n{y_train_resampled.value_counts(normalize=True)}\") # Should be balanced\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during SMOTE: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f645ff-4a3b-4ea6-bbb4-8226939f80bc",
   "metadata": {},
   "source": [
    "EXPLANATION ON 1.4\n",
    "Original training shape: X=(800000, 10), y=(800000,)\n",
    "This shows the size of your training dataset before applying SMOTE.\n",
    "You had 800,000 samples (rows).\n",
    "X_train had 10 feature columns.\n",
    "y_train had 800,000 corresponding labels.\n",
    "Original training class distribution:\n",
    "isFraud\n",
    "0 0.999465: Roughly 99.95% of your original training data was labelled 'Not Fraud' (class 0).\n",
    "1 0.000535: Only about 0.05% of your original training data was labelled 'Fraud' (class 1).\n",
    "This confirms the severe class imbalance that SMOTE is designed to address. The model would likely ignore the tiny 'Fraud' class if trained on this original data.\n",
    "Applying SMOTE (this might take a moment)...\n",
    "This indicates that the smote.fit_resample(X_train, y_train) command started executing. Since n_jobs was removed, it likely ran on a single CPU core.\n",
    "SMOTE application complete.\n",
    "Confirmation that the process finished without errors.\n",
    "Resampled training shape: X=(1599144, 10), y=(1599144,)\n",
    "This shows the size of your training dataset after applying SMOTE (X_train_resampled, y_train_resampled).\n",
    "The number of samples has significantly increased to 1,599,144.\n",
    "Why the increase? SMOTE works by oversampling the minority class. It doesn't remove majority samples. It creates new, synthetic samples for the minority ('Fraud') class until the number of minority samples equals the number of majority samples.\n",
    "The number of features (10) remains the same, as SMOTE only creates new samples (rows), not new features.\n",
    "Resampled training class distribution:\n",
    "isFraud\n",
    "0 0.5: Exactly 50% of the resampled training data is now 'Not Fraud'.\n",
    "1 0.5: Exactly 50% of the resampled training data is now 'Fraud'.\n",
    "This confirms the successful outcome of SMOTE: the training dataset is now perfectly balanced.\n",
    "In essence: SMOTE successfully addressed the class imbalance by generating synthetic 'Fraud' examples, resulting in a larger, balanced training dataset (X_train_resampled, y_train_resampled). This balanced dataset will now be used to train the XGBoost model in the next step, forcing the model to pay equal attention to both 'Fraud' and 'Not Fraud' patterns, which is crucial for achieving good Recall on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d9185f-8cf7-4bce-b144-12eec7d181fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.5: Train Baseline XGBoost Model ---\n",
      "XGBoost Classifier instantiated.\n",
      "Training XGBoost model on resampled data (takes time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [07:16:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model training complete.\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.5: Train Baseline Model (XGBoost)\n",
    "import xgboost as xgb\n",
    "print(\"\\n--- Step 1.5: Train Baseline XGBoost Model ---\")\n",
    "\n",
    "\n",
    "# Ensure resampled training data exists\n",
    "if not 'X_train_resampled' in locals() or not 'y_train_resampled' in locals():\n",
    "     print(\"ERROR: Resampled training data not found.\")\n",
    "     raise NameError(\"Resampled data missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Instantiate the classifier\n",
    "    model_xgb_baseline = xgb.XGBClassifier(\n",
    "        random_state=42,          # For reproducibility of internal randomness\n",
    "        use_label_encoder=False,  # Avoids potential deprecation warnings\n",
    "        eval_metric='logloss'     # Common metric, avoids potential warnings if not set\n",
    "    )\n",
    "    print(\"XGBoost Classifier instantiated.\")\n",
    "    print(\"Training XGBoost model on resampled data (takes time)...\")\n",
    "\n",
    "\n",
    "    # Train the model using the balanced (SMOTE'd) training data\n",
    "    model_xgb_baseline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "\n",
    "    print(\"Baseline model training complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during XGBoost training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfe72d-2b45-4d0a-86fb-db2e0d3df5d2",
   "metadata": {},
   "source": [
    "--- Step 1.5: Train Baseline XGBoost Model ---: Indicates the start of this step.\n",
    "XGBoost Classifier instantiated.: Confirms that the xgb.XGBClassifier(...) line executed correctly, creating the model object before training.\n",
    "Training XGBoost model on resampled data (takes time)...: Shows that the model_xgb_baseline.fit(X_train_resampled, y_train_resampled) command started. This is where the actual learning happens, using the SMOTE-balanced data.\n",
    "UserWarning: [...] Parameters: { \"use_label_encoder\" } are not used.:\n",
    "What it means: XGBoost is telling you that even though you provided the parameter use_label_encoder=False when creating the classifier, this parameter is essentially ignored or not needed by the underlying training mechanism in newer versions, especially when you also specify eval_metric.\n",
    "Why it happens: use_label_encoder was primarily relevant for older versions or specific internal label handling that is now deprecated or automatically handled differently. Setting it to False is the recommended practice to avoid potential future issues, but the library is just letting you know it didn't actively use that specific setting during this particular training run.\n",
    "Is it a problem? No. This is a common warning and can be safely ignored. It does not mean the training failed or is incorrect.\n",
    "Baseline model training complete.: This is the most important message! It confirms that the .fit() process finished successfully without crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a70fc8-8494-4adc-80bb-687e0effa9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1.6: Evaluate Baseline Model on Test Set ---\n",
      "Making predictions on the original (imbalanced) test set...\n",
      "\n",
      "Confusion Matrix (Baseline):\n",
      "[[199141    752]\n",
      " [    13     94]]\n",
      "\n",
      "Classification Report (Baseline):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Fraud (0)     0.9999    0.9962    0.9981    199893\n",
      "    Fraud (1)     0.1111    0.8785    0.1973       107\n",
      "\n",
      "     accuracy                         0.9962    200000\n",
      "    macro avg     0.5555    0.9374    0.5977    200000\n",
      " weighted avg     0.9995    0.9962    0.9977    200000\n",
      "\n",
      "\n",
      "Recall for Fraud Class (1): 0.8785\n",
      "\n",
      "Checking against MVP Recall Target (0.75)...\n",
      ">>> Level 1 MVP Recall target MET! :) <<<\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 1.6: Evaluate Baseline Model\n",
    "from sklearn.metrics import classification_report, recall_score, confusion_matrix\n",
    "print(\"\\n--- Step 1.6: Evaluate Baseline Model on Test Set ---\")\n",
    "\n",
    "\n",
    "# Ensure model and test data exist\n",
    "if not 'model_xgb_baseline' in locals(): raise NameError(\"Baseline model not trained.\")\n",
    "if not 'X_test' in locals() or not 'y_test' in locals(): raise NameError(\"Test data not available.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Making predictions on the original (imbalanced) test set...\")\n",
    "    # Use the trained model to predict on the unseen X_test\n",
    "    y_pred_baseline = model_xgb_baseline.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Baseline):\")\n",
    "    # Rows = Actual (0: Not Fraud, 1: Fraud)\n",
    "    # Cols = Predicted (0: Not Fraud, 1: Fraud)\n",
    "    cm = confusion_matrix(y_test, y_pred_baseline)\n",
    "    print(cm)\n",
    "    # You can manually interpret: TN=cm[0,0], FP=cm[0,1], FN=cm[1,0], TP=cm[1,1]\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report (Baseline):\")\n",
    "    print(classification_report(y_test, y_pred_baseline, target_names=['Not Fraud (0)', 'Fraud (1)'], digits=4))\n",
    "\n",
    "\n",
    "    # Calculate recall specifically for the positive 'Fraud' class (label=1)\n",
    "    recall_fraud = recall_score(y_test, y_pred_baseline, pos_label=1)\n",
    "    print(f\"\\nRecall for Fraud Class (1): {recall_fraud:.4f}\")\n",
    "\n",
    "\n",
    "    # Check MVP goal\n",
    "    MVP_RECALL_TARGET = 0.75 # Our defined target\n",
    "    print(f\"\\nChecking against MVP Recall Target ({MVP_RECALL_TARGET})...\")\n",
    "    if recall_fraud >= MVP_RECALL_TARGET:\n",
    "        print(f\">>> Level 1 MVP Recall target MET! :) <<<\")\n",
    "    else:\n",
    "        print(f\">>> Level 1 MVP Recall target NOT MET. :( <<<\")\n",
    "        print(f\"    (Further tuning in Level 2 or revisiting preprocessing might be needed)\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during baseline model evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8dc19-fcaa-453e-9bdb-dc2dd2f35b7e",
   "metadata": {},
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef243146-29b4-47c6-aa97-b69d9bd0404e",
   "metadata": {},
   "source": [
    "--- LEVEL 2 (Refined Model) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feb8177c-4fa5-4c6a-929c-093b6c96400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.1: Feature Engineering (Example) ---\n",
      "Original X_train shape: (800000, 10)\n",
      "Added 'amt_ratio_orig' feature.\n",
      "X_train_fe shape after FE: (800000, 11)\n",
      "X_test_fe shape after FE: (200000, 11)\n",
      "\n",
      "First 3 rows of X_train_fe with new feature(s):\n",
      "        step    amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "22807      8  47249.07            0.0            0.00      1040812.95   \n",
      "18282      8   2551.07        25280.0        22728.93            0.00   \n",
      "916408    43  72301.09        72908.0          606.91            0.00   \n",
      "\n",
      "        newbalanceDest  type_CASH_OUT  type_DEBIT  type_PAYMENT  \\\n",
      "22807       1930535.01           True       False         False   \n",
      "18282             0.00          False       False          True   \n",
      "916408       123056.71           True       False         False   \n",
      "\n",
      "        type_TRANSFER  amt_ratio_orig  \n",
      "22807           False    4.724907e+10  \n",
      "18282           False    1.009126e-01  \n",
      "916408          False    9.916757e-01  \n",
      "\n",
      "Re-applying SMOTE to feature-engineered training data...\n",
      "SMOTE application complete on feature-engineered data.\n",
      "X_train_fe_resampled shape: (1599144, 11)\n",
      "y_train_fe_resampled shape: (1599144,)\n",
      "\n",
      "Resampled class distribution:\n",
      "isFraud\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.1: Feature Engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE # Re-import if in new notebook\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2.1: Feature Engineering (Example) ---\")\n",
    "\n",
    "\n",
    "# Ensure original train/test splits exist from Level 1\n",
    "if not 'X_train' in locals() or not 'X_test' in locals() or not 'y_train' in locals():\n",
    "     print(\"ERROR: Original train/test splits (X_train, X_test, y_train) not found.\")\n",
    "     raise NameError(\"Original splits missing.\")\n",
    "\n",
    "\n",
    "# Create copies to avoid modifying original dataframes\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe = X_test.copy()\n",
    "print(f\"Original X_train shape: {X_train_fe.shape}\")\n",
    "\n",
    "\n",
    "# Example 1: Amount / Origin Balance Ratio\n",
    "epsilon = 1e-6 # Small value to prevent division by zero\n",
    "X_train_fe['amt_ratio_orig'] = (X_train_fe['amount'] / (X_train_fe['oldbalanceOrg'] + epsilon)).fillna(0)\n",
    "X_test_fe['amt_ratio_orig'] = (X_test_fe['amount'] / (X_test_fe['oldbalanceOrg'] + epsilon)).fillna(0)\n",
    "print(\"Added 'amt_ratio_orig' feature.\")\n",
    "\n",
    "\n",
    "# Example 2: Balance Discrepancy (Optional - can add complexity)\n",
    "# X_train_fe['orig_bal_discrepancy'] = (X_train_fe['oldbalanceOrg'] - X_train_fe['newbalanceOrig']) - X_train_fe['amount']\n",
    "# X_test_fe['orig_bal_discrepancy'] = (X_test_fe['oldbalanceOrg'] - X_test_fe['newbalanceOrig']) - X_test_fe['amount']\n",
    "# print(\"Added 'orig_bal_discrepancy' feature.\")\n",
    "\n",
    "\n",
    "print(f\"X_train_fe shape after FE: {X_train_fe.shape}\")\n",
    "print(f\"X_test_fe shape after FE: {X_test_fe.shape}\")\n",
    "print(\"\\nFirst 3 rows of X_train_fe with new feature(s):\")\n",
    "print(X_train_fe.head(3))\n",
    "\n",
    "\n",
    "# --- Re-apply SMOTE on the NEW feature-engineered training data ---\n",
    "# We need to balance the training set *after* adding features\n",
    "print(\"\\nRe-applying SMOTE to feature-engineered training data...\")\n",
    "try:\n",
    "    smote_fe = SMOTE(random_state=50)\n",
    "    # Use the feature-engineered training data (X_train_fe) and original y_train\n",
    "    X_train_fe_resampled, y_train_fe_resampled = smote_fe.fit_resample(X_train_fe, y_train)\n",
    "    print(\"SMOTE application complete on feature-engineered data.\")\n",
    "    print(f\"X_train_fe_resampled shape: {X_train_fe_resampled.shape}\")\n",
    "    print(f\"y_train_fe_resampled shape: {y_train_fe_resampled.shape}\")\n",
    "    print(f\"\\nResampled class distribution:\\n{y_train_fe_resampled.value_counts(normalize=True)}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during SMOTE on feature-engineered data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579e5749-01a1-4782-b6aa-21226dfb54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.2: Build Scikit-learn Pipeline ---\n",
      "Pipeline object created successfully:\n",
      "Pipeline(steps=[('xgboost',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric='logloss',\n",
      "                               feature_types=None, feature_weights=None,\n",
      "                               gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.2: Build Scikit-learn Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler # Optional: if you wanted to add scaling\n",
    "import xgboost as xgb # Re-import if needed\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2.2: Build Scikit-learn Pipeline ---\")\n",
    "\n",
    "\n",
    "# Define the steps for the pipeline\n",
    "pipeline_steps = [\n",
    "    # Optional Step: Scaling (StandardScaler makes features have zero mean, unit variance)\n",
    "    # Though XGBoost is not sensitive to scaling, it's good practice if comparing models\n",
    "    # ('scaler', StandardScaler()),\n",
    "\n",
    "\n",
    "    # Final Step: The XGBoost Classifier\n",
    "    # Use the step name 'xgboost' to match parameter names later in tuning\n",
    "    ('xgboost', xgb.XGBClassifier(random_state=50, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "\n",
    "\n",
    "# Create the Pipeline object\n",
    "pipeline_obj = Pipeline(steps=pipeline_steps)\n",
    "\n",
    "\n",
    "print(\"Pipeline object created successfully:\")\n",
    "print(pipeline_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3eb35c7-705f-4ed4-8e83-43cd92cc0a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: hyperopt in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: ray[tune] in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (2.45.0)\n",
      "Requirement already satisfied: click>=7.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (8.1.8)\n",
      "Requirement already satisfied: filelock in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (3.18.0)\n",
      "Requirement already satisfied: jsonschema in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (4.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (1.1.0)\n",
      "Requirement already satisfied: packaging in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (24.2)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (6.30.2)\n",
      "Requirement already satisfied: pyyaml in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (6.0.2)\n",
      "Requirement already satisfied: requests in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2.32.3)\n",
      "Requirement already satisfied: pandas in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2.2.3)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (19.0.1)\n",
      "Requirement already satisfied: fsspec in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from ray[tune]) (2024.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Requirement already satisfied: pyaml>=16.9 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (25.1.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-optimize) (1.6.1)\n",
      "Requirement already satisfied: six in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (1.17.0)\n",
      "Requirement already satisfied: networkx>=2.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (3.4.2)\n",
      "Requirement already satisfied: future in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: tqdm in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (3.1.1)\n",
      "Requirement already satisfied: py4j in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from hyperopt) (0.10.9.9)\n",
      "Requirement already satisfied: colorama in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from click>=7.0->ray[tune]) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from jsonschema->ray[tune]) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from referencing>=0.28.4->jsonschema->ray[tune]) (4.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from pandas->ray[tune]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from pandas->ray[tune]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from pandas->ray[tune]) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\ai prep\\projects\\project 1\\realtime-fraud-detection-api\\p1env\\lib\\site-packages (from requests->ray[tune]) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install ray[tune] scikit-optimize hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8ea13-cae7-49d2-92f1-8579fe37f4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70edb50-5cb9-4361-a4a8-379bdd9cfdb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.tune.integration.sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Sub-Step 2.3: Hyperparameter Tuning with Ray Tune\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtune\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TuneSearchCV\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m randint, uniform\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mray\u001b[39;00m \u001b[38;5;66;03m# Import ray if needed for initialization (sometimes automatic)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ray.tune.integration.sklearn'"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.3: Hyperparameter Tuning with Ray Tune\n",
    "from ray.tune.integration.sklearn import TuneSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import ray # Import ray if needed for initialization (sometimes automatic)\n",
    "\n",
    "\n",
    "# Optional: Initialize Ray explicitly (might help manage resources)\n",
    "# if not ray.is_initialized():\n",
    "#    ray.init(ignore_reinit_error=True, num_cpus=4) # Adjust num_cpus if needed\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2.3: Hyperparameter Tuning with Ray Tune ---\")\n",
    "\n",
    "\n",
    "# Define parameter search space for XGBoost within the pipeline\n",
    "# Prefix parameters with step name ('xgboost__')\n",
    "param_distributions = {\n",
    "    'xgboost__n_estimators': randint(150, 500),        # Number of trees\n",
    "    'xgboost__max_depth': randint(4, 12),             # Max depth of trees\n",
    "    'xgboost__learning_rate': uniform(0.01, 0.2),    # Step size shrinkage (loc=0.01, scale=0.2 -> range 0.01 to 0.21)\n",
    "    'xgboost__subsample': uniform(0.6, 0.4),         # Fraction of samples used per tree (range 0.6 to 1.0)\n",
    "    'xgboost__colsample_bytree': uniform(0.6, 0.4),  # Fraction of features used per tree (range 0.6 to 1.0)\n",
    "    # Add other XGBoost params if desired, e.g., gamma, reg_alpha, reg_lambda\n",
    "}\n",
    "\n",
    "\n",
    "# Setup TuneSearchCV\n",
    "tune_search = TuneSearchCV(\n",
    "    pipeline_obj,              # The pipeline with (optional scaler and) XGBoost\n",
    "    param_distributions,\n",
    "    n_trials=15,               # Increase trials for better search (e.g., 15-30), but takes longer\n",
    "    scoring='recall',          # Optimize for recall of the positive class (Fraud=1)\n",
    "    cv=3,                      # 3-fold cross-validation\n",
    "    random_state=50,\n",
    "    n_jobs=-1,                 # Use available cores\n",
    "    verbose=2                  # Show more output during tuning\n",
    ")\n",
    "\n",
    "print(\"Starting Ray Tune search (THIS CAN TAKE SIGNIFICANT TIME)...\")\n",
    "# Fit TuneSearchCV on the FEATURE-ENGINEERED, SMOTE-RESAMPLED training data\n",
    "# Ensure these variables exist from Sub-Step 2.1\n",
    "if not 'X_train_fe_resampled' in locals() or not 'y_train_fe_resampled' in locals():\n",
    "     print(\"ERROR: Feature-engineered resampled training data not found.\")\n",
    "     raise NameError(\"Feature-engineered resampled data missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    tune_search.fit(X_train_fe_resampled, y_train_fe_resampled) # Fit on the balanced, feature-engineered training set!\n",
    "\n",
    "\n",
    "    print(\"\\nRay Tune search finished.\")\n",
    "    print(f\"Best parameters found: {tune_search.best_params_}\")\n",
    "    print(f\"Best cross-validation recall score during search: {tune_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "    # Access the best pipeline refitted on the whole training data\n",
    "    best_pipeline = tune_search.best_estimator_\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Ray Tune search: {e}\")\n",
    "    # Check Ray logs if available for more details\n",
    "    raise\n",
    "# Optional: Shutdown Ray if initialized manually\n",
    "# ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594eebd-1267-4fad-8d31-84573a9ab997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray error checking 1\n",
    "import sys\n",
    "print(\"Python Executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b6c91-df9b-44b1-835f-28dc5192c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray error checking 1\n",
    "import importlib\n",
    "try:\n",
    "    ray_spec = importlib.util.find_spec(\"ray\")\n",
    "    if ray_spec:\n",
    "        print(\"Found 'ray' installation at:\", ray_spec.origin)\n",
    "        # Try importing base ray and tune\n",
    "        import ray\n",
    "        print(\"Successfully imported base 'ray'. Version:\", ray.__version__)\n",
    "        import ray.tune\n",
    "        print(\"Successfully imported 'ray.tune'.\")\n",
    "        # Check if the specific module path exists conceptually\n",
    "        if hasattr(ray.tune, 'integration') and hasattr(ray.tune.integration, 'sklearn'):\n",
    "            print(\"'ray.tune.integration.sklearn' path seems accessible.\")\n",
    "            # Try the final import again\n",
    "            from ray.tune.integration.sklearn import TuneSearchCV\n",
    "            print(\"SUCCESS: Imported TuneSearchCV!\")\n",
    "        else:\n",
    "             print(\"ERROR: 'ray.tune.integration.sklearn' structure not found in this ray version.\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: 'ray' package not found in this environment.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR during import: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de7a9c-bc37-4ac6-a7b9-33816906a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray checking 3\n",
    "import ray.tune\n",
    "import inspect\n",
    "\n",
    "print(\"--- Inspecting ray.tune submodules ---\")\n",
    "found_locations = []\n",
    "possible_locations = ['sklearn', 'integration', 'integration.sklearn'] # Things we've tried or expect\n",
    "\n",
    "for name, obj in inspect.getmembers(ray.tune):\n",
    "    if inspect.ismodule(obj):\n",
    "        print(f\"Found submodule: ray.tune.{name}\")\n",
    "        # Check if it contains TuneSearchCV or is one of our target paths\n",
    "        if name in possible_locations:\n",
    "            found_locations.append(f\"ray.tune.{name}\")\n",
    "        try:\n",
    "            if hasattr(obj, 'TuneSearchCV'):\n",
    "                 print(f\"  >>> Found TuneSearchCV in: ray.tune.{name}\")\n",
    "                 found_locations.append(f\"ray.tune.{name}.TuneSearchCV\")\n",
    "            # Check one level deeper for integration.sklearn\n",
    "            if name == 'integration' and hasattr(obj, 'sklearn'):\n",
    "                 if hasattr(obj.sklearn, 'TuneSearchCV'):\n",
    "                      print(f\"  >>> Found TuneSearchCV in: ray.tune.integration.sklearn\")\n",
    "                      found_locations.append(f\"ray.tune.integration.sklearn.TuneSearchCV\")\n",
    "        except Exception as e:\n",
    "            print(f\"  (Could not inspect submodule {name} fully: {e})\")\n",
    "\n",
    "\n",
    "if not found_locations:\n",
    "    print(\"\\nERROR: Could not find TuneSearchCV in expected ray.tune submodules.\")\n",
    "    print(\"This suggests the 'ray[tune]' installation might be incomplete or corrupted,\")\n",
    "    print(\"or TuneSearchCV is located elsewhere in Ray v2.45.0.\")\n",
    "else:\n",
    "    print(\"\\nPossible locations containing TuneSearchCV or relevant modules:\")\n",
    "    for loc in set(found_locations): # Use set to remove duplicates\n",
    "         print(f\"- {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c67a1c-d374-4ab1-8b2b-49a18e8b3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray checking 4\n",
    "from ray.tune.integration.sklearn import TuneSearchCV\n",
    "print(\"Successfully imported TuneSearchCV!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95145a4-6b35-4d9f-8080-956da6528aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.3: Hyperparameter Tuning with RandomizedSearchCV ---\n",
      "Setting up RandomizedSearchCV with 15 iterations...\n",
      "Starting RandomizedSearch search (THIS CAN TAKE SIGNIFICANT TIME)...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [07:27:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomizedSearch search finished.\n",
      "Best parameters found: {'xgboost__colsample_bytree': np.float64(0.9157080755356058), 'xgboost__learning_rate': np.float64(0.08288386811018128), 'xgboost__max_depth': 8, 'xgboost__n_estimators': 472, 'xgboost__subsample': np.float64(0.8673350275106719)}\n",
      "Best cross-validation recall score during search: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# --- MODIFIED Sub-Step 2.3: Hyperparameter Tuning with RandomizedSearchCV ---\n",
    "# from ray.tune.integration.sklearn import TuneSearchCV # REMOVED RAY IMPORT\n",
    "from sklearn.model_selection import RandomizedSearchCV # USE SKLEARN'S TOOL\n",
    "from scipy.stats import randint, uniform\n",
    "# import ray # No longer needed for this step\n",
    "\n",
    "print(\"\\n--- Step 2.3: Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
    "\n",
    "# --- Ensure pipeline_obj exists from Step 2.2 ---\n",
    "if not 'pipeline_obj' in locals():\n",
    "     print(\"ERROR: 'pipeline_obj' (Pipeline object from Step 2.2) not found.\")\n",
    "     raise NameError(\"'pipeline_obj' is missing.\")\n",
    "\n",
    "# --- Define parameter search space (SAME AS BEFORE) ---\n",
    "# Prefix parameters with step name ('xgboost__')\n",
    "param_distributions = {\n",
    "    'xgboost__n_estimators': randint(150, 500),        # Number of trees\n",
    "    'xgboost__max_depth': randint(4, 12),             # Max depth of trees\n",
    "    'xgboost__learning_rate': uniform(0.01, 0.2),    # Step size shrinkage (range 0.01 to 0.21)\n",
    "    'xgboost__subsample': uniform(0.6, 0.4),         # Fraction of samples used per tree (range 0.6 to 1.0)\n",
    "    'xgboost__colsample_bytree': uniform(0.6, 0.4),  # Fraction of features used per tree (range 0.6 to 1.0)\n",
    "}\n",
    "\n",
    "# --- Setup RandomizedSearchCV (INSTEAD OF TuneSearchCV) ---\n",
    "n_iterations = 15 # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "print(f\"Setting up RandomizedSearchCV with {n_iterations} iterations...\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline_obj,              # The pipeline object\n",
    "    param_distributions=param_distributions, # The distributions to sample from\n",
    "    n_iter=n_iterations,       # Number of combinations to try (like n_trials)\n",
    "    scoring='recall',          # Metric to optimize for (use 'recall_macro' or specific scorer if needed)\n",
    "    cv=3,                      # 3-fold cross-validation\n",
    "    random_state=50,           # For reproducibility (use 42 if preferred from plan)\n",
    "    n_jobs=-1,                 # Use all available CPU cores for CV folds\n",
    "    verbose=2                  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Starting RandomizedSearch search (THIS CAN TAKE SIGNIFICANT TIME)...\")\n",
    "# Ensure feature-engineered, resampled training data exists from Step 2.1\n",
    "if not 'X_train_fe_resampled' in locals() or not 'y_train_fe_resampled' in locals():\n",
    "     print(\"ERROR: Feature-engineered resampled training data not found.\")\n",
    "     raise NameError(\"Feature-engineered resampled data missing.\")\n",
    "\n",
    "try:\n",
    "    # Fit RandomizedSearchCV on the FEATURE-ENGINEERED, SMOTE-RESAMPLED training data\n",
    "    random_search.fit(X_train_fe_resampled, y_train_fe_resampled)\n",
    "\n",
    "    print(\"\\nRandomizedSearch search finished.\")\n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "    print(f\"Best cross-validation recall score during search: {random_search.best_score_:.4f}\")\n",
    "\n",
    "    # Access the best pipeline refitted on the whole training data\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during RandomizedSearch search: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad7ed826-9a84-4fea-9b69-4abc7d6b1592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.4: Evaluate Tuned Model on Test Set ---\n",
      "Making predictions with the tuned pipeline on the feature-engineered test set...\n",
      "\n",
      "Confusion Matrix (Tuned):\n",
      "[[199683    210]\n",
      " [    13     94]]\n",
      "\n",
      "Classification Report (Tuned):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Fraud (0)     0.9999    0.9989    0.9994    199893\n",
      "    Fraud (1)     0.3092    0.8785    0.4574       107\n",
      "\n",
      "     accuracy                         0.9989    200000\n",
      "    macro avg     0.6546    0.9387    0.7284    200000\n",
      " weighted avg     0.9996    0.9989    0.9992    200000\n",
      "\n",
      "\n",
      "Recall for Fraud Class (1) - Tuned: 0.8785\n",
      "(Recall Baseline was: 0.8785)\n",
      ">>> Recall did NOT improve vs Baseline.\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.4: Evaluate Tuned Model\n",
    "from sklearn.metrics import classification_report, recall_score, confusion_matrix\n",
    "print(\"\\n--- Step 2.4: Evaluate Tuned Model on Test Set ---\")\n",
    "\n",
    "\n",
    "# Ensure best_pipeline and X_test_fe exist\n",
    "if not 'best_pipeline' in locals():\n",
    "     print(\"ERROR: 'best_pipeline' not found. Tuning may have failed.\")\n",
    "     raise NameError(\"'best_pipeline' not defined.\")\n",
    "if not 'X_test_fe' in locals() or not 'y_test' in locals():\n",
    "     print(\"ERROR: Feature-engineered test data (X_test_fe) or y_test not found.\")\n",
    "     raise NameError(\"Test data missing.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Making predictions with the tuned pipeline on the feature-engineered test set...\")\n",
    "    # Predict using the best pipeline on X_test_fe\n",
    "    y_pred_tuned = best_pipeline.predict(X_test_fe)\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Tuned):\")\n",
    "    print(confusion_matrix(y_test, y_pred_tuned)) # Compare against original y_test\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report (Tuned):\")\n",
    "    print(classification_report(y_test, y_pred_tuned, target_names=['Not Fraud (0)', 'Fraud (1)'], digits=4))\n",
    "\n",
    "\n",
    "    # Calculate recall for the positive 'Fraud' class\n",
    "    recall_fraud_tuned = recall_score(y_test, y_pred_tuned, pos_label=1)\n",
    "    print(f\"\\nRecall for Fraud Class (1) - Tuned: {recall_fraud_tuned:.4f}\")\n",
    "\n",
    "\n",
    "    # Optional: Compare with baseline recall if stored from L1\n",
    "    if 'recall_fraud' in locals():\n",
    "          print(f\"(Recall Baseline was: {recall_fraud:.4f})\")\n",
    "          if recall_fraud_tuned > recall_fraud: print(\">>> Recall IMPROVED vs Baseline.\")\n",
    "          else: print(\">>> Recall did NOT improve vs Baseline.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during tuned model evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e90cd87-5c03-457c-a26e-46aa1302ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.5: Save Best Pipeline & Commit ---\n",
      "Saved best pipeline object to 'best_fraud_pipeline.joblib'\n"
     ]
    }
   ],
   "source": [
    "#Sub-Step 2.5: Save Artifacts & Commit Level 2 Completion\n",
    "\n",
    "import joblib # Or use pickle\n",
    "print(\"\\n--- Step 2.5: Save Best Pipeline & Commit ---\")\n",
    "\n",
    "\n",
    "pipeline_filename = \"best_fraud_pipeline.joblib\"\n",
    "if 'best_pipeline' in locals():\n",
    "     try:\n",
    "         joblib.dump(best_pipeline, pipeline_filename)\n",
    "         print(f\"Saved best pipeline object to '{pipeline_filename}'\")\n",
    "     except Exception as e:\n",
    "         print(f\"ERROR saving pipeline: {e}\")\n",
    "else:\n",
    "     print(\"WARNING: 'best_pipeline' object not found, cannot save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a809b6b-cf8c-41ab-a291-5c627d5f040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~upyterlab (E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9fb7c-60a9-45ff-b78b-aadba8a7bf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection-env",
   "language": "python",
   "name": "fraud-detection-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
