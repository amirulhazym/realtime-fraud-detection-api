{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4727bf-9166-484c-b5aa-7666d8a403ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ray Tune Core API Implementation ---\n",
      "Imports successful.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports ---\n",
    "print(\"--- Ray Tune Core API Implementation ---\")\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn for pipeline, splitting, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score\n",
    "# Optional: If you added StandardScaler or other preprocessing in the pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Imbalanced-learn for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ray Tune\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "print(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5071932-97a8-46c6-adae-8ebb28993bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete.\n",
      "X_train_fe shape: (800000, 11)\n",
      "y_train shape: (800000,)\n",
      "X_test_fe shape: (200000, 11)\n",
      "y_test shape: (200000,)\n",
      "Pipeline object created: Pipeline(steps=[('xgboost',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None, device=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric='logloss',\n",
      "                               feature_types=None, feature_weights=None,\n",
      "                               gamma=None, grow_policy=None,\n",
      "                               importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=None, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, multi_strategy=None,\n",
      "                               n_estimators=None, n_jobs=None,\n",
      "                               num_parallel_tree=None, ...))])\n"
     ]
    }
   ],
   "source": [
    "# Combined Data Loading, Preprocessing, Splitting, Feature Engineering, and Pipeline Creation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import gc # For memory management\n",
    "\n",
    "# --- Load and Sample Data ---\n",
    "hf_csv_url = \"hf://datasets/MatrixIA/FraudData/FraudData.csv\"\n",
    "df_full = pd.read_csv(hf_csv_url) # Requires huggingface_hub\n",
    "sample_size = 1000000\n",
    "df = df_full.head(sample_size).copy()\n",
    "del df_full\n",
    "gc.collect()\n",
    "\n",
    "# --- Preprocessing (Selection, Encoding, Define X/y) ---\n",
    "features_to_keep = ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "target = 'isFraud'\n",
    "df_processed = df[features_to_keep + [target]].copy()\n",
    "df_processed = pd.get_dummies(df_processed, columns=['type'], drop_first=True, dtype=int)\n",
    "X = df_processed.drop(target, axis=1)\n",
    "y = df_processed[target]\n",
    "del df # Clean up intermediate dataframe\n",
    "gc.collect()\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "del X, y # Clean up intermediate variables\n",
    "gc.collect()\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe = X_test.copy()\n",
    "epsilon = 1e-6\n",
    "X_train_fe['amt_ratio_orig'] = (X_train_fe['amount'] / (X_train_fe['oldbalanceOrg'] + epsilon)).fillna(0)\n",
    "X_test_fe['amt_ratio_orig'] = (X_test_fe['amount'] / (X_test_fe['oldbalanceOrg'] + epsilon)).fillna(0)\n",
    "del X_train, X_test # Clean up original splits if only feature-engineered ones are needed next\n",
    "gc.collect()\n",
    "\n",
    "# --- Build Pipeline Object ---\n",
    "pipeline_steps = [\n",
    "    # Add scaler here if needed: e.g., ('scaler', StandardScaler()),\n",
    "    ('xgboost', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "pipeline_obj = Pipeline(steps=pipeline_steps)\n",
    "\n",
    "# --- Verification (Optional - Can be commented out after confirmation) ---\n",
    "print(f\"Data Preparation Complete.\")\n",
    "print(f\"X_train_fe shape: {X_train_fe.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test_fe shape: {X_test_fe.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Pipeline object created: {pipeline_obj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081059a3-ecd8-4177-aa21-fa170f0fdcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray Tune training function 'train_fraud_model_ray' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Define Ray Tune Training Function ---\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import recall_score\n",
    "from ray import tune\n",
    "\n",
    "def train_fraud_model_ray(config):\n",
    "    \"\"\"Trains and validates one trial for Ray Tune.\"\"\"\n",
    "    # Assumes X_train_fe and y_train are accessible in the outer scope\n",
    "\n",
    "    # 1. Internal Train/Validation Split\n",
    "    try:\n",
    "        X_tune_train, X_tune_val, y_tune_train, y_tune_val = train_test_split(\n",
    "            X_train_fe, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "    except NameError:\n",
    "         print(\"ERROR in train_fraud_model_ray: Could not access X_train_fe or y_train.\")\n",
    "         tune.report(recall=0.0, error=\"Data loading failed\")\n",
    "         return\n",
    "\n",
    "    # 2. Apply SMOTE to Internal Training Split\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_tune_train_res, y_tune_train_res = smote.fit_resample(X_tune_train, y_tune_train)\n",
    "\n",
    "    # 3. Prepare DMatrix\n",
    "    dtrain = xgb.DMatrix(X_tune_train_res, label=y_tune_train_res)\n",
    "    dval = xgb.DMatrix(X_tune_val, label=y_tune_val)\n",
    "\n",
    "    # 4. Train using xgb.train API\n",
    "    evals_result = {}\n",
    "    try:\n",
    "        bst = xgb.train(\n",
    "            params=config,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=config.get(\"n_estimators\", 100), # Use n_estimators from config\n",
    "            evals=[(dval, \"eval\")],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "\n",
    "        # 5. Evaluate on Internal Validation Set\n",
    "        y_pred_val_proba = bst.predict(dval)\n",
    "        y_pred_val_labels = (y_pred_val_proba > 0.5).astype(int) # Threshold probabilities\n",
    "        validation_recall = recall_score(y_tune_val, y_pred_val_labels, pos_label=1, zero_division=0)\n",
    "\n",
    "        # 6. Report Results to Ray Tune\n",
    "        tune.report(recall=validation_recall, done=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during training/evaluation in trial: {e}\")\n",
    "        tune.report(recall=0.0, error=str(e), done=True) # Report failure\n",
    "\n",
    "print(\"Ray Tune training function 'train_fraud_model_ray' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79dc190-d800-4234-946f-dd18a8e85a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining Ray Tune search space and tuner...\n",
      "Ray Tune Tuner configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Define Ray Tune Search Space and Tuner ---\n",
    "from ray import tune # Ensure tune is imported\n",
    "\n",
    "print(\"\\nDefining Ray Tune search space and tuner...\")\n",
    "\n",
    "# Define parameter search space using tune.* functions\n",
    "# These keys MUST match the parameters expected by xgb.train within your training function\n",
    "param_space = {\n",
    "    # XGBoost Training Parameters (params argument for xgb.train)\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"recall\"], # Track multiple metrics if desired\n",
    "    \"eta\": tune.loguniform(1e-4, 1e-1),  # Learning rate (log scale)\n",
    "    \"max_depth\": tune.randint(4, 12), # Integer between 4 and 11 (exclusive upper bound for randint)\n",
    "    \"min_child_weight\": tune.choice([1, 2, 3, 4, 5]), # Choose from discrete values\n",
    "    \"subsample\": tune.uniform(0.6, 1.0), # Float between 0.6 and 1.0\n",
    "    \"colsample_bytree\": tune.uniform(0.6, 1.0), # Float between 0.6 and 1.0\n",
    "    # Explicitly include n_estimators here for the training function to access\n",
    "    \"n_estimators\": tune.randint(150, 501), # Integer between 150 and 500 (exclusive upper bound)\n",
    "    \"random_state\": 42 # Fixed seed for XGBoost internal randomness (passed in config)\n",
    "}\n",
    "\n",
    "# Configure the Tuner\n",
    "tuner = tune.Tuner(\n",
    "    train_fraud_model_ray, # The trainable function defined in the previous step\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"recall\",       # Optimize based on the 'recall' key reported by tune.report\n",
    "        mode=\"max\",            # We want to maximize recall\n",
    "        num_samples=15,       # Number of different hyperparameter combinations to try\n",
    "        # Optional: Add scheduler for early stopping (uncomment to use)\n",
    "        # from ray.tune.schedulers import ASHAScheduler\n",
    "        # scheduler=ASHAScheduler(metric=\"recall\", mode=\"max\", grace_period=5, reduction_factor=2),\n",
    "    ),\n",
    "    param_space=param_space, # The search space defined above\n",
    "    # Optional: Add run_config for naming experiment, storage etc.\n",
    "    # run_config=ray.air.RunConfig(name=\"fraud_xgb_tune\")\n",
    ")\n",
    "\n",
    "print(\"Ray Tune Tuner configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154eab87-df7c-4cad-bd20-0cf16215d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Ray Tune experiment (tuner.fit())...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 09:07:00,950\tERROR services.py:1362 -- Failed to start the dashboard , return code 3221226505\n",
      "2025-05-02 09:07:00,951\tERROR services.py:1387 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2025-05-02 09:07:00,960\tERROR services.py:1431 -- \n",
      "The last 20 lines of C:\\Users\\amiru\\AppData\\Local\\Temp\\ray\\session_2025-05-02_09-06-58_748577_10872\\logs\\dashboard.log (it contains the error message from the dashboard): \n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\ray\\dashboard\\dashboard.py\", line 247, in <module>\n",
      "    logging_utils.redirect_stdout_stderr_if_needed(\n",
      "  File \"E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\ray\\_private\\logging_utils.py\", line 48, in redirect_stdout_stderr_if_needed\n",
      "    sys.stderr = open_log(stderr_fileno, unbuffered=True, closefd=False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"E:\\AI Prep\\Projects\\Project 1\\realtime-fraud-detection-api\\p1env\\Lib\\site-packages\\ray\\_private\\utils.py\", line 446, in open_log\n",
      "    stream = open(path, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [WinError 6] The handle is invalid\n",
      "\n",
      "\n",
      "2025-05-02 09:07:01,159\tINFO worker.py:1888 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Run Ray Tune Experiment ---\n",
    "import ray # Ensure ray is imported\n",
    "import time # To time the experiment\n",
    "\n",
    "print(\"\\nStarting Ray Tune experiment (tuner.fit())...\")\n",
    "# Initialize Ray explicitly - ensures clean start and helps manage resources\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "# log_to_driver=False keeps the notebook output cleaner by suppressing worker logs\n",
    "# include_dashboard=False can save some overhead if you don't need the Ray dashboard\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False, include_dashboard=False)\n",
    "\n",
    "start_tune_time = time.time()\n",
    "best_result = None # Initialize variable\n",
    "\n",
    "try:\n",
    "    # This starts the hyperparameter tuning process\n",
    "    results = tuner.fit()\n",
    "    end_tune_time = time.time()\n",
    "    print(f\"\\nRay Tune experiment finished. Total time: {end_tune_time - start_tune_time:.2f} seconds\")\n",
    "\n",
    "    # Check if any trials resulted in errors\n",
    "    if results.errors:\n",
    "        print(\"\\nWARNING: Some trials encountered errors:\")\n",
    "        # Iterate through results to find trials with errors\n",
    "        for i, trial_result in enumerate(results):\n",
    "            if trial_result.error:\n",
    "                # Attempt to access trial ID if available (might depend on Ray version/structure)\n",
    "                trial_id = trial_result.trial_id if hasattr(trial_result, 'trial_id') else f\"Trial_{i}\"\n",
    "                print(f\"- {trial_id}: {trial_result.error}\")\n",
    "\n",
    "    # Get the best result based on the specified metric and mode\n",
    "    # Use try-except in case no trial completed successfully\n",
    "    try:\n",
    "         best_result = results.get_best_result(metric=\"recall\", mode=\"max\")\n",
    "         if best_result:\n",
    "             print(\"\\n--- Best Trial Information ---\")\n",
    "             print(f\"Best trial config: {best_result.config}\")\n",
    "             # Access metrics reported via tune.report()\n",
    "             print(f\"Best trial final validation recall: {best_result.metrics.get('recall', 'N/A')}\")\n",
    "             # Log path might contain more detailed logs/checkpoints if configured\n",
    "             # print(f\"Best trial log path: {best_result.path}\")\n",
    "         else:\n",
    "              print(\"\\nWARNING: No successful trials found or best result could not be determined.\")\n",
    "              print(\"Check individual trial errors or tuning configuration.\")\n",
    "\n",
    "    except Exception as e_best:\n",
    "         print(f\"\\nERROR retrieving best result: {e_best}\")\n",
    "         print(\"Possibly no trials completed successfully. Check trial errors above.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!! UNEXPECTED ERROR during tuner.fit(): {e}\")\n",
    "    # Consider adding traceback print here for debugging if needed\n",
    "    # import traceback\n",
    "    # traceback.print_exc()\n",
    "    # raise # Optionally re-raise the exception\n",
    "finally:\n",
    "    # Always try to shut down Ray when done or if an error occurs\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "        print(\"\\nRay runtime shut down.\")\n",
    "\n",
    "# --- End of Cell 5 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09658486-1130-436d-8e28-daed5412e7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud-detection-env",
   "language": "python",
   "name": "fraud-detection-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
